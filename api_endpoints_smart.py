# coding: utf-8
"""
æ™ºèƒ½APIç«¯ç‚¹ï¼ˆå®Œå…¨æ¨¡æ‹Ÿsmart_batch_auto.pyçš„å·¥ä½œæµç¨‹ï¼‰

å·¥ä½œæµç¨‹ï¼š
1. å‚æ•°ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆBIZä¸“å±ï¼‰
2. æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§ï¼Œå¤±æ•ˆæ—¶è‡ªåŠ¨é‡æ–°æ•è·
3. ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å‚æ•°
4. ä¸‹è½½ HTMLã€è·å–ç»Ÿè®¡æ•°æ®
5. ä¿å­˜ä¸º JSON å’Œ CSV
6. æœ€åä¸Šä¼ åˆ°æ•°æ®åº“
"""

from flask import request, jsonify
from datetime import datetime, timedelta
import logging
import os
import sys
import re
import json
import time
import importlib.util

# å¯¼å…¥ç°æœ‰åŠŸèƒ½
from smart_batch_fetch import (
    extract_appmsg_token_from_cookie,
    extract_biz_from_url,
    save_to_csv,
    save_to_json
)
from download_full_html import download_full_html_with_stats
from extract_stats_from_html import extract_stats_from_html
from wechatarticles import ArticlesInfo
from db_operations import (
    get_or_create_account,
    save_article
)

logger = logging.getLogger(__name__)


def _write_params_to_config(biz_params):
    """
    å°†BIZå‚æ•°å†™å…¥params/new_wechat_config.py
    è¿™æ ·download_full_html.pyå°±èƒ½è¯»å–åˆ°æ­£ç¡®çš„å‚æ•°
    """
    from datetime import datetime
    
    config_content = f'''# coding: utf-8
# ç”±api_endpoints_smart.pyè‡ªåŠ¨ç”Ÿæˆ
# BIZ: {biz_params.get('BIZ', '')}
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

COOKIE = '{biz_params.get('COOKIE', '')}'

KEY = '{biz_params.get('KEY', '')}'

PASS_TICKET = '{biz_params.get('PASS_TICKET', '')}'

UIN = '{biz_params.get('UIN', '')}'

DEVICETYPE = '{biz_params.get('DEVICETYPE', 'UnifiedPCWindows')}'

CLIENTVERSION = '{biz_params.get('CLIENTVERSION', '')}'

BIZ = '{biz_params.get('BIZ', '')}'
'''
    
    with open('params/new_wechat_config.py', 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    # é‡æ–°åŠ è½½æ¨¡å—
    import importlib
    import params.new_wechat_config
    importlib.reload(params.new_wechat_config)
    
    logger.info(f"   âœ… å·²æ›´æ–°params/new_wechat_config.py")


def check_params_validity(biz, biz_params):
    """
    æ£€æŸ¥å‚æ•°æ˜¯å¦æœ‰æ•ˆ
    
    Parameters
    ----------
    biz : str
        å…¬ä¼—å·BIZ
    biz_params : dict
        BIZå‚æ•°
    
    Returns
    -------
    bool
        Trueè¡¨ç¤ºæœ‰æ•ˆï¼ŒFalseè¡¨ç¤ºå¤±æ•ˆ
    """
    try:
        logger.info(f"ğŸ” æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§...")
        
        # æå–appmsg_token
        appmsg_token = extract_appmsg_token_from_cookie(biz_params['COOKIE'])
        if not appmsg_token:
            logger.warning(f"   âš ï¸  æ— æ³•æå–appmsg_token")
            return False
        
        # æµ‹è¯•APIè°ƒç”¨
        import requests
        test_url = (
            f"https://mp.weixin.qq.com/mp/profile_ext?"
            f"action=getmsg&"
            f"__biz={biz}&"
            f"f=json&"
            f"offset=0&"
            f"count=1&"
            f"is_ok=1&"
            f"scene=124&"
            f"uin={biz_params['UIN']}&"
            f"key={biz_params['KEY']}&"
            f"pass_ticket={biz_params['PASS_TICKET']}&"
            f"wxtoken=&"
            f"appmsg_token={appmsg_token}&"
            f"x5=0"
        )
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Cookie': biz_params['COOKIE'],
            'Referer': f'https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124',
        }
        
        response = requests.get(test_url, headers=headers, timeout=10)
        data = response.json()
        
        ret = data.get('ret')
        errmsg = data.get('errmsg', '')
        
        if ret == -3 or 'no session' in errmsg.lower():
            logger.warning(f"   âŒ å‚æ•°å·²å¤±æ•ˆ: {errmsg}")
            return False
        elif ret != 0:
            logger.warning(f"   âš ï¸  APIè¿”å›é”™è¯¯: {errmsg}")
            return False
        
        logger.info(f"   âœ… å‚æ•°æœ‰æ•ˆ")
        return True
        
    except Exception as e:
        logger.error(f"   âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False


def load_biz_params_from_file(biz):
    """
    ä»æœ¬åœ°æ–‡ä»¶åŠ è½½BIZä¸“å±å‚æ•°ï¼ˆå®Œå…¨æ¨¡æ‹Ÿsmart_batch_auto.pyï¼‰
    
    Parameters
    ----------
    biz : str
        å…¬ä¼—å·BIZ
    
    Returns
    -------
    dict or None
        å‚æ•°å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
    """
    try:
        biz_config_file = f"params/biz_{biz}/config.py"
        
        if not os.path.exists(biz_config_file):
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°BIZä¸“å±é…ç½®: {biz_config_file}")
            return None
        
        logger.info(f"ğŸ“‚ åŠ è½½BIZä¸“å±é…ç½®: biz_{biz}/config.py")
        
        # åŠ¨æ€å¯¼å…¥BIZä¸“å±é…ç½®
        spec = importlib.util.spec_from_file_location("biz_config", biz_config_file)
        biz_config = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(biz_config)
        
        return {
            'COOKIE': biz_config.COOKIE,
            'KEY': biz_config.KEY,
            'PASS_TICKET': biz_config.PASS_TICKET,
            'UIN': biz_config.UIN,
            'BIZ': biz_config.BIZ
        }
    except Exception as e:
        logger.error(f"âŒ åŠ è½½BIZé…ç½®å¤±è´¥: {e}")
        return None


def fetch_articles_from_api(biz, biz_params, start_date=None, end_date=None):
    """
    ä»å¾®ä¿¡APIè·å–æ–‡ç« åˆ—è¡¨ï¼ˆå®Œå…¨æ¨¡æ‹Ÿsmart_batch_auto.pyï¼‰
    
    Parameters
    ----------
    biz : str
        å…¬ä¼—å·BIZ
    biz_params : dict
        BIZä¸“å±å‚æ•°
    start_date : datetime, optional
        å¼€å§‹æ—¥æœŸ
    end_date : datetime, optional
        ç»“æŸæ—¥æœŸ
    
    Returns
    -------
    list or dict
        æ–‡ç« åˆ—è¡¨ï¼Œæˆ–é”™è¯¯ä¿¡æ¯å­—å…¸
    """
    logger.info(f"ğŸ“¡ ä»å¾®ä¿¡APIè·å–æ–‡ç« åˆ—è¡¨...")
    
    # æå–appmsg_token
    appmsg_token = extract_appmsg_token_from_cookie(biz_params['COOKIE'])
    if not appmsg_token:
        logger.error("âŒ æ— æ³•ä»Cookieæå–appmsg_token")
        return {'error': 'invalid_params', 'message': 'æ— æ³•æå–appmsg_token'}
    
    # æ„é€ è¯·æ±‚å¤´
    import requests
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 MicroMessenger/3.4.0',
        'Cookie': biz_params['COOKIE'],
        'Referer': f'https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'X-Requested-With': 'XMLHttpRequest',
    }
    
    all_articles = []
    offset = 0
    count = 10
    has_more = True
    
    while has_more:
        api_url = (
            f"https://mp.weixin.qq.com/mp/profile_ext?"
            f"action=getmsg&"
            f"__biz={biz}&"
            f"f=json&"
            f"offset={offset}&"
            f"count={count}&"
            f"is_ok=1&"
            f"scene=124&"
            f"uin={biz_params['UIN']}&"
            f"key={biz_params['KEY']}&"
            f"pass_ticket={biz_params['PASS_TICKET']}&"
            f"wxtoken=&"
            f"appmsg_token={appmsg_token}&"
            f"x5=0"
        )
        
        try:
            logger.info(f"   è·å–ç¬¬ {offset//count + 1} é¡µ...")
            
            response = requests.get(api_url, headers=headers, timeout=10)
            data = response.json()
            
            if data.get('ret') != 0:
                errmsg = data.get('errmsg', 'Unknown')
                if 'no session' in errmsg.lower():
                    logger.error(f"âŒ å‚æ•°å·²å¤±æ•ˆ: {errmsg}")
                    return {'error': 'no_session', 'message': 'å‚æ•°å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°æ•è·'}
                logger.warning(f"   âš ï¸  APIè¿”å›é”™è¯¯: {errmsg}")
                break
            
            # è§£ææ–‡ç« åˆ—è¡¨
            general_msg_list = data.get('general_msg_list', {})
            if isinstance(general_msg_list, str):
                general_msg_list = json.loads(general_msg_list)
            
            msg_list = general_msg_list.get('list', [])
            
            if not msg_list:
                logger.info(f"   âœ… å·²è·å–æ‰€æœ‰æ–‡ç« ")
                break
            
            # å¤„ç†æ¯æ¡æ¶ˆæ¯
            for msg in msg_list:
                comm_msg_info = msg.get('comm_msg_info', {})
                app_msg_ext_info = msg.get('app_msg_ext_info', {})
                
                if not app_msg_ext_info:
                    continue
                
                publish_time = comm_msg_info.get('datetime', 0)
                article_date = datetime.fromtimestamp(publish_time)
                
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                if end_date and article_date > end_date + timedelta(days=1):
                    continue
                
                if start_date and article_date < start_date:
                    logger.info(f"   ğŸ›‘ é‡åˆ°æ—©äºå¼€å§‹æ—¥æœŸçš„æ–‡ç«  ({article_date.date()})ï¼Œåœæ­¢è·å–")
                    has_more = False
                    break
                
                # ä¸»æ–‡ç« 
                article_url = app_msg_ext_info.get('content_url', '').replace('\\/', '/')
                article_title = app_msg_ext_info.get('title', '')
                
                if article_url:
                    article = {
                        'title': article_title,
                        'url': article_url,
                        'digest': app_msg_ext_info.get('digest', ''),
                        'cover': app_msg_ext_info.get('cover', ''),
                        'publish_time': publish_time,
                        'publish_date': article_date.strftime('%Y-%m-%d'),
                    }
                    all_articles.append(article)
                
                # å¤šå›¾æ–‡æ¶ˆæ¯
                multi_app_msg_item_list = app_msg_ext_info.get('multi_app_msg_item_list', [])
                if multi_app_msg_item_list:
                    logger.info(f"   ğŸ“‘ å‘ç° {len(multi_app_msg_item_list)} ç¯‡å¤šå›¾æ–‡")
                    for item in multi_app_msg_item_list:
                        sub_article = {
                            'title': item.get('title', ''),
                            'url': item.get('content_url', '').replace('\\/', '/'),
                            'digest': item.get('digest', ''),
                            'cover': item.get('cover', ''),
                            'publish_time': publish_time,
                            'publish_date': article_date.strftime('%Y-%m-%d'),
                        }
                        if sub_article['url']:
                            all_articles.append(sub_article)
            
            offset += count
            time.sleep(1 + (offset % 3))  # éšæœºå»¶è¿Ÿ
            
        except Exception as e:
            logger.error(f"   âŒ è·å–å¤±è´¥: {e}")
            break
    
    logger.info(f"âœ… ä»APIè·å– {len(all_articles)} ç¯‡æ–‡ç« ")
    return all_articles


def fetch_articles_smart():
    """
    æ™ºèƒ½æ‰¹é‡è·å–æ–‡ç« ï¼ˆå®Œå…¨æ¨¡æ‹Ÿsmart_batch_auto.py + æ™ºèƒ½å¢é‡ï¼‰
    
    è¯·æ±‚ä½“ï¼š
    {
        "account_name": "å…¬ä¼—å·åç§°",
        "article_url": "ä»»æ„ä¸€ç¯‡æ–‡ç« URL",
        "start_date": "2024-12-01",
        "end_date": "2024-12-10"
    }
    
    å·¥ä½œæµç¨‹ï¼š
    1. ä»URLæå–BIZ
    2. æ£€æŸ¥æ•°æ®åº“å·²æœ‰å“ªäº›æ—¥æœŸçš„æ–‡ç« ï¼ˆæ™ºèƒ½å¢é‡ï¼‰
    3. åŠ è½½æœ¬åœ°BIZä¸“å±å‚æ•°æ–‡ä»¶
    4. æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§ï¼Œå¤±æ•ˆæ—¶è‡ªåŠ¨æ‰“å¼€å¾®ä¿¡+æ–‡ç« é‡æ–°æ•è·
    5. åªè·å–ç¼ºå¤±æ—¥æœŸçš„æ–‡ç« 
    6. ä¸‹è½½HTMLã€è·å–ç»Ÿè®¡æ•°æ®
    7. ä¿å­˜ä¸ºJSONå’ŒCSV
    8. ä¸Šä¼ åˆ°æ•°æ®åº“
    9. è¿”å›å®Œæ•´æ—¥æœŸèŒƒå›´çš„æ–‡ç« ï¼ˆæ•°æ®åº“å·²æœ‰+æ–°è·å–ï¼‰
    """
    try:
        # è§£æè¯·æ±‚
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'}), 400
        
        account_name = data.get('account_name')
        article_url = data.get('article_url')
        start_date_str = data.get('start_date')
        end_date_str = data.get('end_date')
        
        if not article_url:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°: article_url'}), 400
        
        logger.info(f"ğŸ“¥ æ”¶åˆ°æ™ºèƒ½æ‰¹é‡è¯·æ±‚: å…¬ä¼—å·={account_name}")
        
        # 1. ä»URLæå–BIZ
        biz = extract_biz_from_url(article_url)
        if not biz:
            return jsonify({'success': False, 'error': 'æ— æ³•ä»URLæå–BIZ'}), 400
        logger.info(f"âœ… ä»URLæå–BIZ: {biz}")
        
        # åˆ›å»ºæˆ–æ›´æ–°è´¦å·
        get_or_create_account(biz, account_name)
        
        # 2. è§£ææ—¥æœŸ
        start_date = datetime.strptime(start_date_str, '%Y-%m-%d') if start_date_str else None
        end_date = datetime.strptime(end_date_str, '%Y-%m-%d') if end_date_str else None
        
        # 3. æ™ºèƒ½å¢é‡ï¼šæ£€æŸ¥æ•°æ®åº“å·²æœ‰å“ªäº›æ—¥æœŸçš„æ–‡ç« 
        logger.info(f"ğŸ” æ£€æŸ¥æ•°æ®åº“å·²æœ‰æ•°æ®...")
        from database import get_db_session
        from models import Article
        
        existing_dates = set()
        missing_dates = []
        
        if start_date and end_date:
            with get_db_session() as session:
                # è·å–è¯¥BIZåœ¨æ—¥æœŸèŒƒå›´å†…æ‰€æœ‰æ–‡ç« çš„å‘å¸ƒæ—¥æœŸ
                existing_articles = session.query(Article.publish_date).filter(
                    Article.biz == biz,
                    Article.publish_date >= start_date.strftime('%Y-%m-%d'),
                    Article.publish_date <= end_date.strftime('%Y-%m-%d')
                ).all()
                
                for row in existing_articles:
                    if row.publish_date:
                        if isinstance(row.publish_date, str):
                            existing_dates.add(row.publish_date)
                        else:
                            existing_dates.add(row.publish_date.strftime('%Y-%m-%d'))
                
                # æ£€æŸ¥æ¯ä¸€å¤©æ˜¯å¦éƒ½æœ‰æ•°æ®
                current_date = start_date
                while current_date <= end_date:
                    date_str = current_date.strftime('%Y-%m-%d')
                    if date_str not in existing_dates:
                        missing_dates.append(date_str)
                    current_date += timedelta(days=1)
        
        if existing_dates:
            logger.info(f"   âœ… æ•°æ®åº“å·²æœ‰ {len(existing_dates)} å¤©çš„æ•°æ®: {sorted(existing_dates)}")
        
        if not missing_dates:
            # æ‰€æœ‰æ—¥æœŸéƒ½æœ‰æ•°æ®ï¼Œç›´æ¥ä»æ•°æ®åº“è¿”å›
            logger.info(f"   âœ… æ‰€æœ‰æ—¥æœŸéƒ½æœ‰æ•°æ®ï¼Œç›´æ¥ä»æ•°æ®åº“è¿”å›")
            
            from db_operations import get_articles_by_filters
            db_articles = get_articles_by_filters(biz, start_date, end_date, None)
            
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': True,
                    'total': len(db_articles),
                    'new_fetched': 0,
                    'articles': db_articles
                }
            })
        
        logger.info(f"   âš ï¸  ç¼ºå¤± {len(missing_dates)} å¤©çš„æ•°æ®: {missing_dates}")
        logger.info(f"   ğŸ“¡ éœ€è¦ä»å¾®ä¿¡APIè·å–ç¼ºå¤±æ•°æ®...")
        
        # 4. åŠ è½½æœ¬åœ°BIZä¸“å±å‚æ•°
        logger.info(f"ğŸ“‚ åŠ è½½æœ¬åœ°BIZä¸“å±å‚æ•°...")
        biz_params = load_biz_params_from_file(biz)
        
        # 5. æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§
        params_valid = False
        if biz_params:
            params_valid = check_params_validity(biz, biz_params)
        
        # 6. å¦‚æœå‚æ•°ä¸å­˜åœ¨æˆ–å·²å¤±æ•ˆï¼Œè§¦å‘è‡ªåŠ¨æ•è·ï¼ˆè‡ªåŠ¨æ‰“å¼€å¾®ä¿¡+æ–‡ç« ï¼‰
        if not biz_params or not params_valid:
            logger.warning(f"âš ï¸  å‚æ•°{'ä¸å­˜åœ¨' if not biz_params else 'å·²å¤±æ•ˆ'}ï¼Œå¼€å§‹è‡ªåŠ¨æ•è·...")
            logger.info(f"ğŸ¤– è‡ªåŠ¨æ‰“å¼€å¾®ä¿¡å¹¶æ‰“å¼€æ–‡ç« ...")
            
            # è‡ªåŠ¨æ‰“å¼€å¾®ä¿¡ä¸­çš„æ–‡ç« é“¾æ¥
            from wechat_automation import auto_open_article_in_wechat
            if not auto_open_article_in_wechat(article_url):
                logger.warning(f"   âš ï¸  è‡ªåŠ¨æ‰“å¼€æ–‡ç« å¤±è´¥ï¼Œç»§ç»­å°è¯•æ•è·...")
            
            # å¯åŠ¨ä»£ç†æ•è·
            from api_server import ProxyManager
            if not ProxyManager.start_proxy_and_capture(article_url, biz=biz, timeout=120):
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·å¤±è´¥ï¼Œè¯·ç¡®ä¿å¾®ä¿¡å·²æ­£å¸¸è¿è¡Œ'
                }), 500
            
            time.sleep(2)
            
            # é‡æ–°åŠ è½½å‚æ•°
            biz_params = load_biz_params_from_file(biz)
            if not biz_params:
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·åä»æ— æ³•åŠ è½½'
                }), 500
            
            # å†æ¬¡æ£€æŸ¥æœ‰æ•ˆæ€§
            params_valid = check_params_validity(biz, biz_params)
            if not params_valid:
                return jsonify({
                    'success': False,
                    'error': 'æ–°æ•è·çš„å‚æ•°ä»ç„¶æ— æ•ˆ'
                }), 500
        
        logger.info(f"âœ… å‚æ•°æœ‰æ•ˆï¼Œå¼€å§‹è·å–æ–‡ç« ")
        
        # 7. ä»å¾®ä¿¡APIè·å–æ–‡ç« åˆ—è¡¨ï¼ˆåªè·å–ç¼ºå¤±æ—¥æœŸçš„ï¼‰
        articles = fetch_articles_from_api(biz, biz_params, start_date, end_date)
        
        if isinstance(articles, dict) and articles.get('error'):
            # å¦‚æœæ˜¯å‚æ•°å¤±æ•ˆé”™è¯¯ï¼Œå†æ¬¡å°è¯•é‡æ–°æ•è·
            if articles.get('error') == 'no_session':
                logger.warning(f"âš ï¸  è·å–æ–‡ç« æ—¶æ£€æµ‹åˆ°å‚æ•°å¤±æ•ˆï¼Œé‡æ–°æ•è·...")
                
                # è‡ªåŠ¨æ‰“å¼€å¾®ä¿¡ä¸­çš„æ–‡ç« 
                from wechat_automation import auto_open_article_in_wechat
                auto_open_article_in_wechat(article_url)
                
                from api_server import ProxyManager
                if ProxyManager.start_proxy_and_capture(article_url, biz=biz, timeout=120):
                    time.sleep(2)
                    biz_params = load_biz_params_from_file(biz)
                    if biz_params:
                        # é‡è¯•è·å–æ–‡ç« 
                        articles = fetch_articles_from_api(biz, biz_params, start_date, end_date)
                        if isinstance(articles, dict) and articles.get('error'):
                            return jsonify({
                                'success': False,
                                'error': articles.get('message', 'è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥'),
                                'need_recapture': True
                            }), 500
                    else:
                        return jsonify({
                            'success': False,
                            'error': 'é‡æ–°æ•è·åä»æ— æ³•åŠ è½½å‚æ•°'
                        }), 500
                else:
                    return jsonify({
                        'success': False,
                        'error': 'å‚æ•°æ•è·å¤±è´¥'
                    }), 500
            else:
                return jsonify({
                    'success': False,
                    'error': articles.get('message', 'è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥')
                }), 500
        
        if not articles:
            # æ²¡æœ‰æ–°æ–‡ç« ï¼Œä½†å¯èƒ½æ•°æ®åº“æœ‰æ—§æ–‡ç« 
            logger.info(f"   â„¹ï¸  æ²¡æœ‰è·å–åˆ°æ–°æ–‡ç« ")
            
            from db_operations import get_articles_by_filters
            db_articles = get_articles_by_filters(biz, start_date, end_date, None)
            
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': True,
                    'total': len(db_articles),
                    'new_fetched': 0,
                    'articles': db_articles
                }
            })
        
        # 8. æ‰¹é‡ä¸‹è½½HTMLå¹¶ä»HTMLä¸­æå–ç»Ÿè®¡æ•°æ®ï¼ˆä½¿ç”¨å‚æ•°åŒ–è¯·æ±‚ï¼‰
        logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡ä¸‹è½½HTMLå¹¶æå–ç»Ÿè®¡æ•°æ®ï¼ˆå…± {len(articles)} ç¯‡æ–°æ–‡ç« ï¼‰...")
        logger.info(f"   ğŸ”§ ä½¿ç”¨å‚æ•°åŒ–è¯·æ±‚æ–¹å¼ï¼ˆä»HTMLä¸­æå–ç»Ÿè®¡æ•°æ®ï¼‰")
        
        # âœ… å…³é”®ï¼šå°†BIZå‚æ•°å†™å…¥new_wechat_config.pyï¼Œä¾›download_full_htmlä½¿ç”¨
        _write_params_to_config(biz_params)
        
        results = []
        success_count = 0
        
        for i, article in enumerate(articles, 1):
            try:
                article_url_item = article.get('url', '')
                article_title = article.get('title', '')
                publish_date = article.get('publish_date', '')
                
                logger.info(f"   [{i}/{len(articles)}] {article_title[:40]}...")
                
                # ä½¿ç”¨å‚æ•°åŒ–è¯·æ±‚ä¸‹è½½HTMLå¹¶æå–ç»Ÿè®¡æ•°æ®
                # å‚æ•°å·²ç»å†™å…¥params/new_wechat_config.pyï¼Œdownload_full_htmlä¼šè‡ªåŠ¨è¯»å–
                download_result = download_full_html_with_stats(
                    article_url_item,
                    article_title,
                    publish_date,
                    account_name=account_name,
                    output_dir="articles_html"
                )
                
                html_file_path = download_result.get('filepath', '')
                stats = download_result.get('stats', {})
                
                if download_result.get('success') and stats:
                    success_count += 1
                    read_num = stats.get('read_num', 0)
                    old_like_count = stats.get('old_like_count', 0)
                    share_count = stats.get('share_count', 0)
                    comment_count = stats.get('comment_count', 0)
                    logger.info(f"      âœ… é˜…è¯»: {read_num} | ç‚¹èµ: {old_like_count} | åˆ†äº«: {share_count} | è¯„è®º: {comment_count}")
                else:
                    logger.warning(f"      âš ï¸  ä¸‹è½½æˆ–æå–ç»Ÿè®¡æ•°æ®å¤±è´¥: {download_result.get('error', '')}")
                
                # è½¬æ¢ç»Ÿè®¡æ•°æ®æ ¼å¼
                def safe_int(val):
                    try:
                        return int(val) if val else 0
                    except:
                        return 0
                
                # åˆå¹¶æ•°æ®
                result = {
                    **article,
                    'local_html_path': html_file_path,
                    'read_count': safe_int(stats.get('read_num')),
                    'like_count': safe_int(stats.get('like_count')),  # å–œæ¬¢/æ”¶è—ï¼ˆçˆ±å¿ƒï¼‰
                    'old_like_count': safe_int(stats.get('old_like_count')),  # ç‚¹èµï¼ˆå¤§æ‹‡æŒ‡ï¼‰
                    'share_count': safe_int(stats.get('share_count')),
                    'comment_count': safe_int(stats.get('comment_count')),
                    'nickname': stats.get('nickname', ''),
                    'user_name': stats.get('user_name', ''),
                    'success': download_result.get('success', False),
                    'method': 'html_extraction'
                }
                results.append(result)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                if i < len(articles):
                    time.sleep(2)
                
            except Exception as e:
                logger.error(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                results.append({
                    **article,
                    'read_count': 0,
                    'like_count': 0,
                    'old_like_count': 0,
                    'share_count': 0,
                    'comment_count': 0,
                    'success': False,
                    'error': str(e)
                })
        
        logger.info(f"âœ… æ‰¹é‡è·å–å®Œæˆ: æˆåŠŸ {success_count}/{len(articles)}")
        
        # 9. ä¿å­˜ä¸ºJSONå’ŒCSV
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"articles_{timestamp}.csv"
        json_filename = f"articles_{timestamp}.json"
        
        save_to_csv(results, csv_filename)
        save_to_json(results, json_filename)
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜: {csv_filename}, {json_filename}")
        
        # 10. ä¸Šä¼ åˆ°æ•°æ®åº“
        logger.info(f"ğŸ“¤ ä¸Šä¼ åˆ°æ•°æ®åº“...")
        uploaded_count = 0
        
        for result in results:
            try:
                article_data = {
                    'biz': biz,
                    'url': result.get('url'),
                    'title': result.get('title'),
                    'html_content': None,  # HTMLå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                    'publish_date': result.get('publish_date'),
                    'read_count': result.get('read_count', 0),
                    'like_count': result.get('like_count', 0),
                    'old_like_count': result.get('old_like_count', 0),
                    'share_count': result.get('share_count', 0),
                    'comment_count': result.get('comment_count', 0),
                    'local_html_path': result.get('local_html_path', '')
                }
                
                save_article(article_data)
                uploaded_count += 1
                
            except Exception as e:
                logger.warning(f"   âš ï¸  ä¸Šä¼ å¤±è´¥: {result.get('title', '')[:30]} - {e}")
        
        logger.info(f"âœ… å·²ä¸Šä¼  {uploaded_count}/{len(results)} ç¯‡æ–°æ–‡ç« åˆ°æ•°æ®åº“")
        
        # 11. ä»æ•°æ®åº“è·å–å®Œæ•´æ—¥æœŸèŒƒå›´çš„æ–‡ç« ï¼ˆå·²æœ‰+æ–°è·å–ï¼‰
        logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–å®Œæ•´æ—¥æœŸèŒƒå›´çš„æ–‡ç« ...")
        from db_operations import get_articles_by_filters
        all_articles = get_articles_by_filters(biz, start_date, end_date, None)
        
        logger.info(f"âœ… è¿”å›å®Œæ•´æ•°æ®: {len(all_articles)} ç¯‡æ–‡ç« ï¼ˆåŒ…å«å·²æœ‰+æ–°è·å–ï¼‰")
        
        # 12. è¿”å›ç»“æœ
        return jsonify({
            'success': True,
            'data': {
                'account_name': account_name,
                'biz': biz,
                'from_cache': False,
                'total': len(all_articles),
                'new_fetched': uploaded_count,
                'existing_in_db': len(all_articles) - uploaded_count,
                'csv_file': csv_filename,
                'json_file': json_filename,
                'articles': all_articles  # è¿”å›å®Œæ•´æ—¥æœŸèŒƒå›´çš„æ‰€æœ‰æ–‡ç« 
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500
