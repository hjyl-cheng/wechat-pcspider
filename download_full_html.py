# -*- coding: utf-8 -*-
"""
ä¸‹è½½åŒ…å«ç»Ÿè®¡æ•°æ®çš„å®Œæ•´HTMLï¼ˆå«è¯„è®ºåŒºæ•°æ®ï¼‰
"""

import re
import os
import requests
from bs4 import BeautifulSoup


def download_full_html_with_stats(article_url, title, publish_date, 
                                   account_name=None,
                                   output_dir="articles_html",
                                   inject_comments=False,
                                   articles_info=None):
    """
    ä¸‹è½½åŒ…å«ç»Ÿè®¡æ•°æ®çš„å®Œæ•´HTML
    
    ä½¿ç”¨å‚æ•°åŒ–è¯·æ±‚è·å–åŒ…å«ç»Ÿè®¡æ•°æ®çš„HTMLï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶
    
    Parameters
    ----------
    article_url : str
        æ–‡ç«  URL
    title : str
        æ–‡ç« æ ‡é¢˜ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
    publish_date : str
        å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    account_name : str, optional
        å…¬ä¼—å·åç§°ï¼ˆç”¨äºåˆ›å»ºå­ç›®å½•ï¼‰
    output_dir : str
        è¾“å‡ºæ ¹ç›®å½•
    inject_comments : bool
        æ˜¯å¦æ³¨å…¥è¯„è®ºåˆ°HTMLï¼ˆé»˜è®¤Falseï¼‰
    articles_info : ArticlesInfo
        ArticlesInfoå®ä¾‹ï¼Œç”¨äºè·å–è¯„è®ºæ•°æ®ï¼ˆinject_comments=Trueæ—¶éœ€è¦ï¼‰
    
    Returns
    -------
    dict
        åŒ…å«æ–‡ä»¶è·¯å¾„å’Œæå–çš„ç»Ÿè®¡æ•°æ®
    """
    # æ„å»ºç›®å½•ç»“æ„ï¼šarticles_html/{å…¬ä¼—å·åç§°}/{æ—¥æœŸ}/
    if account_name:
        # æ¸…ç†å…¬ä¼—å·åç§°ï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        safe_account_name = re.sub(r'[\u003c\u003e:\"/\\\\|?*]', '_', account_name)
        safe_account_name = safe_account_name[:50]  # é™åˆ¶é•¿åº¦
        
        # æå–æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        if publish_date:
            date_str = publish_date  # å·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼
        else:
            from datetime import datetime
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ›å»ºç›®å½•ï¼šarticles_html/{å…¬ä¼—å·}/{æ—¥æœŸ}/
        account_dir = os.path.join(output_dir, safe_account_name)
        date_dir = os.path.join(account_dir, date_str)
        os.makedirs(date_dir, exist_ok=True)
        
        final_output_dir = date_dir
    else:
        # å¦‚æœæ²¡æœ‰å…¬ä¼—å·åç§°ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
        os.makedirs(output_dir, exist_ok=True)
        final_output_dir = output_dir
    
    # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
    safe_title = re.sub(r'[\u003c\u003e:\"/\\\\|?*]', '_', title)
    safe_title = safe_title[:100]  # é™åˆ¶é•¿åº¦
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{safe_title}.html"
    filepath = os.path.join(final_output_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(filepath):
        print(f"      âœ… å®Œæ•´HTMLå·²å­˜åœ¨: {filepath}")
        return {'filepath': filepath, 'exists': True}
    
    try:
        # å¯¼å…¥å‚æ•°
        from params.new_wechat_config import KEY, UIN, PASS_TICKET, COOKIE
        
        # URL è§£ç 
        import html as html_module
        article_url = html_module.unescape(article_url)
        
        # ä»URLæå–å‚æ•°
        __biz = re.findall(r"__biz=(.*?)&", article_url)[0]
        mid = re.findall(r"mid=(.*?)&", article_url)[0]
        idx = re.findall(r"idx=(.*?)&", article_url)[0]
        sn = re.findall(r"sn=([^&#]+)", article_url)[0]
        
        # å¯é€‰å‚æ•°
        chksm_match = re.findall(r"chksm=(.*?)&", article_url)
        chksm = chksm_match[0] if chksm_match else ""
        
        scene_match = re.findall(r"scene=(.*?)#", article_url)
        if not scene_match:
            scene_match = re.findall(r"scene=(.*?)&", article_url)
        scene = scene_match[0] if scene_match else "126"
        
        # æ„é€ å‚æ•°åŒ–è¯·æ±‚
        url = "https://mp.weixin.qq.com/s"
        params = {
            "__biz": __biz,
            "mid": mid,
            "idx": idx,
            "sn": sn,
            "scene": scene,
            "key": KEY,
            "uin": UIN,
            "pass_ticket": PASS_TICKET,
            "devicetype": "Windows",
            "version": "6309091f",
            "lang": "zh_CN",
            "acctmode": "0",
            "ascene": "1",
            "wx_header": "1"
        }
        
        if chksm:
            params["chksm"] = chksm
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cookie': COOKIE
        }
        
        # ç¦ç”¨ä»£ç†
        proxies = {"http": None, "https": None}
        
        print(f"      ğŸ”§ æ­£åœ¨ä¸‹è½½åŒ…å«ç»Ÿè®¡æ•°æ®çš„å®Œæ•´HTML...")
        
        # ä¸‹è½½ HTML
        response = requests.get(url, params=params, headers=headers, proxies=proxies, timeout=15)
        
        if response.status_code == 200:
            html_content = response.text
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç»Ÿè®¡æ•°æ®
            has_read_num = "var read_num" in html_content
            has_like_count = "old_like_count" in html_content
            has_share_count = "share_count" in html_content
            has_comment_count = "comment_count" in html_content
            
            print(f"      ğŸ“Š ç»Ÿè®¡æ•°æ®æ£€æŸ¥:")
            print(f"         - é˜…è¯»æ•°å˜é‡: {'âœ…' if has_read_num else 'âŒ'}")
            print(f"         - ç‚¹èµæ•°å˜é‡: {'âœ…' if has_like_count else 'âŒ'}")
            print(f"         - åˆ†äº«æ•°å˜é‡: {'âœ…' if has_share_count else 'âŒ'}")
            print(f"         - è¯„è®ºæ•°å˜é‡: {'âœ…' if has_comment_count else 'âŒ'}")
            
            # æå–ç»Ÿè®¡æ•°æ®å€¼ï¼ˆç”¨äºè¿”å›ç»™è°ƒç”¨è€…ï¼‰
            stats = {}
            try:
                read_match = re.findall(r"var read_num = '(.*?)'", html_content)
                if not read_match:
                    read_match = re.findall(r"var read_num_new = '(.*?)'", html_content)
                if read_match:
                    stats['read_num'] = read_match[0]
                    # ä¸æ‰“å°é˜…è¯»æ•°ï¼ˆé¿å…é‡å¤ï¼‰
            except:
                pass
            
            try:
                # ç‚¹èµï¼ˆå¤§æ‹‡æŒ‡ğŸ‘ï¼‰
                old_like_match = re.findall(r"old_like_count: '(.*?)'", html_content)
                if old_like_match:
                    stats['old_like_count'] = old_like_match[0]
                    print(f"      âœ… ç‚¹èµæ•°: {stats['old_like_count']}")
            except:
                pass
            
            try:
                # å–œæ¬¢ï¼ˆçˆ±å¿ƒâ¤ï¸ï¼‰
                like_match = re.findall(r"like_count: '(.*?)'", html_content)
                if like_match:
                    stats['like_count'] = like_match[0]
                    # ä¸æ‰“å°å–œæ¬¢æ•°ï¼ˆé¿å…æ··æ·†ï¼‰
            except:
                pass
            
            try:
                share_match = re.findall(r"share_count: '(.*?)'", html_content)
                if share_match:
                    stats['share_count'] = share_match[0]
                    print(f"      âœ… åˆ†äº«æ•°: {stats['share_count']}")
            except:
                pass
            
            try:
                comment_match = re.findall(r"comment_count: '(.*?)'", html_content)
                if comment_match:
                    stats['comment_count'] = comment_match[0]
                    print(f"      âœ… è¯„è®ºæ•°: {stats['comment_count']}")
            except:
                pass
            
            # è§£æHTMLå¹¶å†…è”CSS
            print(f"      ğŸ“¥ æ­£åœ¨å¤„ç†HTML...")
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨ CSS é“¾æ¥
            css_links = soup.find_all('link', rel='stylesheet')
            
            if css_links:
                print(f"      ğŸ“¥ å‘ç° {len(css_links)} ä¸ªå¤–éƒ¨CSSï¼Œæ­£åœ¨ä¸‹è½½...")
                
                # æ”¶é›†æ‰€æœ‰CSSå†…å®¹
                all_css = []
                
                for link in css_links:
                    css_url = link.get('href')
                    if css_url:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if css_url.startswith('//'):
                            css_url = 'https:' + css_url
                        elif css_url.startswith('/'):
                            css_url = 'https://mp.weixin.qq.com' + css_url
                        elif not css_url.startswith('http'):
                            continue
                        
                        try:
                            css_response = requests.get(css_url, headers=headers, proxies=proxies, timeout=10)
                            if css_response.status_code == 200:
                                all_css.append(f"\n/* CSS from: {css_url} */\n")
                                all_css.append(css_response.text)
                                all_css.append("\n")
                        except:
                            pass
                
                # å†…è”CSS
                if all_css:
                    style_tag = soup.new_tag('style')
                    style_tag.string = ''.join(all_css)
                    
                    if soup.head:
                        soup.head.append(style_tag)
                    
                    for link in css_links:
                        link.decompose()
                    
                    print(f"      âœ… å·²å†…è” {len(css_links)} ä¸ªCSSæ–‡ä»¶")
            
            # å¤„ç†å›¾ç‰‡
            img_tags = soup.find_all('img')
            if img_tags:
                replaced_count = 0
                for img in img_tags:
                    data_src = img.get('data-src')
                    if data_src:
                        img['src'] = data_src
                        replaced_count += 1
                
                if replaced_count > 0:
                    print(f"      âœ… å·²æ›¿æ¢ {replaced_count} ä¸ªå›¾ç‰‡çš„ src å±æ€§")
            
            # ä¿å­˜HTML
            html_content = str(soup)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            file_size = os.path.getsize(filepath)
            print(f"      âœ… å®Œæ•´HTMLå·²ä¿å­˜: {filepath}")
            print(f"      ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")
            
            # âœ… æ³¨å…¥ç•™è¨€ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if inject_comments and articles_info:
                print(f"      ğŸ’¬ æ­£åœ¨è·å–å¹¶æ³¨å…¥ç•™è¨€...")
                try:
                    # ä½¿ç”¨æ”¹è¿›çš„æ–¹æ³•è·å–ç•™è¨€
                    from get_comments_improved import get_comments_with_params
                    from params.new_wechat_config import KEY, UIN, PASS_TICKET
                    
                    comments_data = get_comments_with_params(
                        article_url,
                        articles_info.appmsg_token,
                        articles_info.headers['Cookie'],
                        KEY,
                        UIN,
                        PASS_TICKET
                    )
                    
                    if comments_data and comments_data.get('elected_comment'):
                        from inject_comments_dom import inject_comments_direct_render
                        comment_count = comments_data.get('elected_comment_total_cnt', 0)
                        comment_list_len = len(comments_data.get('elected_comment', []))
                        print(f"      âœ… å‡†å¤‡æ¸²æŸ“ {comment_list_len}/{comment_count} æ¡ç•™è¨€")
                        inject_comments_direct_render(filepath, comments_data)
                    else:
                        print(f"      âš ï¸  æœªèƒ½è·å–åˆ°ç•™è¨€æ•°æ®")
                except Exception as e:
                    print(f"      âš ï¸  ç•™è¨€æ³¨å…¥å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            return {
                'filepath': filepath,
                'exists': False,
                'stats': stats,
                'file_size': file_size,
                'success': True
            }
        else:
            print(f"      âŒ HTTP {response.status_code}")
            return {'success': False, 'error': f'HTTP {response.status_code}'}
            
    except Exception as e:
        print(f"      âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys
    
    if len(sys.argv) > 1:
        test_url = sys.argv[1]
    else:
        test_url = input("è¯·è¾“å…¥æ–‡ç« URL: ").strip()
    
    if not test_url:
        print("URL ä¸èƒ½ä¸ºç©º")
        sys.exit(1)
    
    print(f"\næµ‹è¯•ä¸‹è½½åŒ…å«ç»Ÿè®¡æ•°æ®çš„å®Œæ•´HTML...")
    print(f"URL: {test_url}")
    print()
    
    result = download_full_html_with_stats(
        test_url,
        "æµ‹è¯•æ–‡ç« ",
        "2025-12-13"
    )
    
    print("\n" + "="*80)
    print("ä¸‹è½½ç»“æœ:")
    print("="*80)
    
    if result.get('success'):
        print(f"âœ… æˆåŠŸ!")
        print(f"æ–‡ä»¶è·¯å¾„: {result['filepath']}")
        if result.get('stats'):
            print(f"\næå–çš„ç»Ÿè®¡æ•°æ®:")
            for key, value in result['stats'].items():
                print(f"  {key}: {value}")
    else:
        print(f"âŒ å¤±è´¥: {result.get('error')}")
    
    print("="*80)
