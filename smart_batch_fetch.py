# coding: utf-8
"""
æ™ºèƒ½æ‰¹é‡è·å–å…¬ä¼—å·æ–‡ç« ä¿¡æ¯å·¥å…·
åªéœ€è¦ä¸€ç¯‡æ–‡ç«  URLï¼Œè‡ªåŠ¨è·å–è¯¥å…¬ä¼—å·çš„æ‰€æœ‰æ–‡ç« å¹¶æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®

åŠŸèƒ½ï¼š
1. ä»å•ç¯‡æ–‡ç«  URL è‡ªåŠ¨æå–å…¬ä¼—å· BIZ
2. è·å–è¯¥å…¬ä¼—å·çš„æ‰€æœ‰æ–‡ç« åˆ—è¡¨
3. æ‰¹é‡ä¸‹è½½æ–‡ç«  HTML åˆ°æœ¬åœ°
4. æ‰¹é‡è·å–æ–‡ç« ç»Ÿè®¡æ•°æ®ï¼ˆé˜…è¯»æ•°ã€ç‚¹èµæ•°ã€åœ¨çœ‹æ•°ã€è¯„è®ºæ•°ï¼‰
5. è¾“å‡ºå®Œæ•´æ•°æ®ï¼ŒåŒ…å«æœ¬åœ° HTML è·¯å¾„
"""

import re
import time
import json
import csv
import requests
from datetime import datetime, timedelta
from params.new_wechat_config import COOKIE
from wechatarticles import ArticlesInfo


def extract_appmsg_token_from_cookie(cookie):
    """ä» Cookie ä¸­æå– appmsg_token"""
    match = re.search(r'appmsg_token=([^;]+)', cookie)
    if match:
        return match.group(1)
    return None


def extract_biz_from_url(article_url):
    """ä»æ–‡ç«  URL ä¸­æå– BIZ"""
    print(f"   åŸå§‹ URL: {article_url}")
    
    # æ–¹æ³•1: ä»å®Œæ•´ URL ä¸­æå–
    match = re.search(r'[?&]__biz=([^&]+)', article_url)
    if match:
        biz = match.group(1)
        print(f"   âœ… ç›´æ¥æå–åˆ° BIZ: {biz}")
        return biz
    
    # æ–¹æ³•2: å¦‚æœæ˜¯çŸ­é“¾æ¥ï¼Œéœ€è¦å…ˆè®¿é—®è·å–å®Œæ•´ URL
    if 'mp.weixin.qq.com/s/' in article_url:
        print(f"   æ£€æµ‹åˆ°çŸ­é“¾æ¥ï¼Œæ­£åœ¨è®¿é—®è·å–å®Œæ•´ URL...")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.0',
                'Cookie': COOKIE,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            # è®¿é—®çŸ­é“¾æ¥ï¼Œè·å–é‡å®šå‘åçš„å®Œæ•´ URL
            response = requests.get(article_url, headers=headers, allow_redirects=True, timeout=15)
            final_url = response.url
            
            print(f"   é‡å®šå‘åçš„ URL: {final_url[:120]}...")
            
            # ä»é‡å®šå‘åçš„ URL ä¸­æå– BIZ
            match = re.search(r'[?&]__biz=([^&]+)', final_url)
            if match:
                biz = match.group(1)
                print(f"   âœ… ä»é‡å®šå‘ URL æå–åˆ° BIZ: {biz}")
                return biz
            else:
                # å°è¯•ä»å“åº”å†…å®¹ä¸­æå–
                print(f"   URL ä¸­æœªæ‰¾åˆ° BIZï¼Œå°è¯•ä»é¡µé¢å†…å®¹æå–...")
                content = response.text
                match = re.search(r'var\s+biz\s*=\s*["\']([^"\']+)["\']', content)
                if not match:
                    match = re.search(r'"biz"\s*:\s*"([^"]+)"', content)
                if match:
                    biz = match.group(1)
                    print(f"   âœ… ä»é¡µé¢å†…å®¹æå–åˆ° BIZ: {biz}")
                    return biz
                
        except Exception as e:
            print(f"   âš ï¸  è®¿é—®çŸ­é“¾æ¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    return None


def get_profile_url(biz):
    """æ„é€ å…¬ä¼—å·é¦–é¡µ URL"""
    return f"https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124#wechat_redirect"


def download_article_html(article_url, title, publish_date, output_dir="articles_html"):
    """
    ä¸‹è½½æ–‡ç«  HTML å†…å®¹
    ç›´æ¥ä½¿ç”¨ requestsï¼Œè®¾ç½®åˆé€‚çš„ headers
    ä¸‹è½½å¹¶å†…è”å¤–éƒ¨ CSSï¼Œä½¿ HTML æ–‡ä»¶å¯ä»¥ç‹¬ç«‹æ˜¾ç¤º
    
    Parameters
    ----------
    article_url : str
        æ–‡ç«  URL
    title : str
        æ–‡ç« æ ‡é¢˜ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
    publish_date : str
        å‘å¸ƒæ—¥æœŸ
    output_dir : str
        è¾“å‡ºç›®å½•
    
    Returns
    -------
    str
        HTML æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    import os
    import html as html_module
    from bs4 import BeautifulSoup
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
    safe_title = safe_title[:100]  # é™åˆ¶é•¿åº¦
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{publish_date}-{safe_title}.html"
    filepath = os.path.join(output_dir, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(filepath):
        return filepath
    
    try:
        # URL è§£ç ï¼šå°† &amp; æ›¿æ¢ä¸º &
        article_url = html_module.unescape(article_url)
        
        # è®¾ç½® headersï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ç¦ç”¨ä»£ç†
        proxies = {"http": None, "https": None}
        
        # ä¸‹è½½ HTML
        response = requests.get(article_url, headers=headers, proxies=proxies, timeout=15)
        
        if response.status_code == 200:
            html_content = response.text
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
            if "ç¯å¢ƒå¼‚å¸¸" in html_content or len(html_content) < 50000:
                print(f"      âš ï¸  å¯èƒ½è§¦å‘äº†éªŒè¯é¡µé¢ï¼ˆå¤§å°{len(html_content)}å­—èŠ‚ï¼‰")
                # ä»ç„¶ä¿å­˜ï¼Œä½†æ ‡è®°å¯èƒ½æœ‰é—®é¢˜
            
            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨ CSS é“¾æ¥
            css_links = soup.find_all('link', rel='stylesheet')
            
            if css_links:
                print(f"      ğŸ“¥ å‘ç° {len(css_links)} ä¸ªå¤–éƒ¨CSSï¼Œæ­£åœ¨ä¸‹è½½...")
                
                # æ”¶é›†æ‰€æœ‰CSSå†…å®¹
                all_css = []
                
                for i, link in enumerate(css_links, 1):
                    css_url = link.get('href')
                    if css_url:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if css_url.startswith('//'):
                            css_url = 'https:' + css_url
                        elif css_url.startswith('/'):
                            css_url = 'https://mp.weixin.qq.com' + css_url
                        elif not css_url.startswith('http'):
                            continue
                        
                        try:
                            # ä¸‹è½½ CSS
                            css_response = requests.get(css_url, headers=headers, proxies=proxies, timeout=10)
                            if css_response.status_code == 200:
                                all_css.append(f"\n/* CSS from: {css_url} */\n")
                                all_css.append(css_response.text)
                                all_css.append("\n")
                        except Exception as e:
                            print(f"      âš ï¸  ä¸‹è½½CSSå¤±è´¥: {css_url[:50]}... - {e}")
                
                # å¦‚æœæˆåŠŸä¸‹è½½äº†CSSï¼Œæ›¿æ¢å¤–éƒ¨é“¾æ¥ä¸ºå†…è”æ ·å¼
                if all_css:
                    # åˆ›å»ºä¸€ä¸ªæ–°çš„ style æ ‡ç­¾
                    style_tag = soup.new_tag('style')
                    style_tag.string = ''.join(all_css)
                    
                    # åœ¨ head ä¸­æ’å…¥
                    if soup.head:
                        soup.head.append(style_tag)
                    
                    # ç§»é™¤æ‰€æœ‰å¤–éƒ¨CSSé“¾æ¥
                    for link in css_links:
                        link.decompose()
                    
                    print(f"      âœ… å·²å†…è” {len(css_links)} ä¸ªCSSæ–‡ä»¶")
            
            # å¤„ç†å›¾ç‰‡ï¼šå°† data-src æ›¿æ¢åˆ° src
            img_tags = soup.find_all('img')
            if img_tags:
                replaced_count = 0
                for img in img_tags:
                    data_src = img.get('data-src')
                    if data_src:
                        # å°† data-src çš„å€¼è®¾ç½®ä¸º src
                        img['src'] = data_src
                        replaced_count += 1
                
                if replaced_count > 0:
                    print(f"      âœ… å·²æ›¿æ¢ {replaced_count} ä¸ªå›¾ç‰‡çš„ src å±æ€§")
            
            # æ›´æ–° HTML å†…å®¹
            html_content = str(soup)
            
            # ä¿å­˜ HTML
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            return filepath
        else:
            print(f"      âš ï¸  HTTP {response.status_code}")
            return ""
            
    except Exception as e:
        print(f"      âš ï¸  ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""


def fetch_articles_from_profile(biz, start_date=None, end_date=None):
    """
    ä»å…¬ä¼—å·é¦–é¡µè·å–æ–‡ç« åˆ—è¡¨
    
    Parameters
    ----------
    biz : str
        å…¬ä¼—å·çš„ BIZ
    start_date : datetime, optional
        å¼€å§‹æ—¥æœŸ
    end_date : datetime, optional
        ç»“æŸæ—¥æœŸ
    
    Returns
    -------
    list
        æ–‡ç« åˆ—è¡¨
    """
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¡ æ­£åœ¨è·å–å…¬ä¼—å·æ–‡ç« åˆ—è¡¨...")
    print(f"{'='*80}")
    print(f"BIZ: {biz}")
    if start_date and end_date:
        print(f"æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
    print(f"{'='*80}\n")
    
    # ä» Cookie ä¸­æå–å¿…è¦å‚æ•°
    from params.new_wechat_config import UIN, KEY, PASS_TICKET
    
    # æå– appmsg_token
    appmsg_token = extract_appmsg_token_from_cookie(COOKIE)
    if not appmsg_token:
        print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ° appmsg_token")
        appmsg_token = ""
    
    # æ„é€ è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 MicroMessenger/3.4.0',
        'Cookie': COOKIE,
        'Referer': f'https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'X-Requested-With': 'XMLHttpRequest',
    }
    
    # âœ… åˆ›å»º ArticlesInfo å®ä¾‹ï¼ˆç”¨äºè·å–ç•™è¨€æ•°æ®ï¼‰
    try:
        from wechatarticles import ArticlesInfo
        articles_info = ArticlesInfo(appmsg_token, COOKIE)
        print("âœ… ArticlesInfo å®ä¾‹å·²åˆ›å»ºï¼ˆæ”¯æŒç•™è¨€æ³¨å…¥ï¼‰")
    except Exception as e:
        print(f"âš ï¸  ArticlesInfo åˆ›å»ºå¤±è´¥: {e}")
        print("   ç•™è¨€æ³¨å…¥åŠŸèƒ½å°†ä¸å¯ç”¨")
        articles_info = None
    
    all_articles = []
    offset = 0
    count = 10  # æ¯æ¬¡è·å–10ç¯‡
    
    while True:
        # æ„é€  API URL - ä½¿ç”¨çœŸå®å‚æ•°
        api_url = (
            f"https://mp.weixin.qq.com/mp/profile_ext?"
            f"action=getmsg&"
            f"__biz={biz}&"
            f"f=json&"
            f"offset={offset}&"
            f"count={count}&"
            f"is_ok=1&"
            f"scene=124&"
            f"uin={UIN}&"
            f"key={KEY}&"
            f"pass_ticket={PASS_TICKET}&"
            f"wxtoken=&"
            f"appmsg_token={appmsg_token}&"
            f"x5=0"
        )
        
        try:
            print(f"   æ­£åœ¨è·å–ç¬¬ {offset//count + 1} é¡µ...")
            
            response = requests.get(api_url, headers=headers, timeout=10)
            data = response.json()
            
            if data.get('ret') != 0:
                print(f"   âš ï¸  API è¿”å›é”™è¯¯: {data.get('errmsg', 'Unknown error')}")
                print(f"   è¿”å›æ•°æ®: {json.dumps(data, ensure_ascii=False)[:200]}")
                break
            
            # è§£ææ–‡ç« åˆ—è¡¨
            general_msg_list = data.get('general_msg_list', {})
            if isinstance(general_msg_list, str):
                general_msg_list = json.loads(general_msg_list)
            
            msg_list = general_msg_list.get('list', [])
            
            if not msg_list:
                print(f"   âœ… å·²è·å–æ‰€æœ‰æ–‡ç« ")
                break
            
            # å¤„ç†æ¯æ¡æ¶ˆæ¯
            for msg in msg_list:
                comm_msg_info = msg.get('comm_msg_info', {})
                app_msg_ext_info = msg.get('app_msg_ext_info', {})
                
                if not app_msg_ext_info:
                    continue
                
                # è·å–å‘å¸ƒæ—¶é—´
                publish_time = comm_msg_info.get('datetime', 0)
                
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                if start_date and end_date:
                    article_date = datetime.fromtimestamp(publish_time)
                    if article_date < start_date or article_date > end_date + timedelta(days=1):
                        continue
                
                # ä¸»æ–‡ç« 
                article_url = app_msg_ext_info.get('content_url', '').replace('\\/', '/')
                article_title = app_msg_ext_info.get('title', '')
                article_date = datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d')
                
                # ä¸‹è½½æ–‡ç«  HTMLï¼ˆåŒ…å«ç»Ÿè®¡æ•°æ®çš„å®Œæ•´ç‰ˆæœ¬ + ç•™è¨€ï¼‰
                print(f"      æ­£åœ¨ä¸‹è½½å®Œæ•´ HTML: {article_title[:30]}...")
                from download_full_html import download_full_html_with_stats
                full_result = download_full_html_with_stats(
                    article_url, 
                    article_title, 
                    article_date, 
                    output_dir="articles_html",
                    inject_comments=True,  # âœ… å¯ç”¨ç•™è¨€æ³¨å…¥
                    articles_info=articles_info  # âœ… ä¼ å…¥ArticlesInfoå®ä¾‹
                )
                html_file = full_result.get('filepath', '')
                
                # âœ… ç›´æ¥ä½¿ç”¨ä¸‹è½½æ—¶æå–çš„ç»Ÿè®¡æ•°æ®ï¼Œé¿å…é‡å¤è¯·æ±‚
                extracted_stats = full_result.get('stats', {})
                
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                
                article = {
                    'title': article_title,
                    'url': article_url,
                    'local_html_path': html_file,
                    'digest': app_msg_ext_info.get('digest', ''),
                    'cover': app_msg_ext_info.get('cover', ''),
                    'publish_time': publish_time,
                    'publish_date': datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d'),
                    # âœ… ä¿å­˜ä¸‹è½½æ—¶æå–çš„ç»Ÿè®¡æ•°æ®
                    '_extracted_stats': extracted_stats,
                }
                
                if article['url']:
                    all_articles.append(article)
                
                # å¤šå›¾æ–‡æ¶ˆæ¯
                multi_app_msg_item_list = app_msg_ext_info.get('multi_app_msg_item_list', [])
                for item in multi_app_msg_item_list:
                    sub_article_url = item.get('content_url', '').replace('\\/', '/')
                    sub_article_title = item.get('title', '')
                    sub_article_date = datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d')
                    
                    print(f"      æ­£åœ¨ä¸‹è½½å®Œæ•´ HTML: {sub_article_title[:30]}...")
                    sub_full_result = download_full_html_with_stats(sub_article_url, sub_article_title, sub_article_date, output_dir="articles_html")
                    sub_html_file = sub_full_result.get('filepath', '')
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                    sub_article = {
                        'title': sub_article_title,
                        'url': sub_article_url,
                        'local_html_path': sub_html_file,
                        'digest': item.get('digest', ''),
                        'cover': item.get('cover', ''),
                        'publish_time': publish_time,
                        'publish_date': datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d'),
                    }
                    
                    if sub_article['url']:
                        all_articles.append(sub_article)
            
            print(f"   å·²è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
            
            # åªè·å–ç¬¬ä¸€é¡µï¼Œä¸ç»§ç»­å¾ªç¯
            print(f"   âœ… å·²è·å–ç¬¬ä¸€é¡µæ–‡ç« ï¼ˆå¦‚éœ€æ›´å¤šï¼Œå¯ä¿®æ”¹ä»£ç ï¼‰")
            break
            
        except Exception as e:
            print(f"   âŒ è·å–å¤±è´¥: {e}")
            break
    
    print(f"\n{'='*80}")
    print(f"âœ… æ–‡ç« åˆ—è¡¨è·å–å®Œæˆ")
    print(f"{'='*80}")
    print(f"æ€»å…±è·å–: {len(all_articles)} ç¯‡æ–‡ç« ")
    print(f"{'='*80}\n")
    
    return all_articles


def get_article_stats(article_url, articles_info=None, use_html_extraction=True):
    """
    è·å–å•ç¯‡æ–‡ç« çš„ç»Ÿè®¡æ•°æ®
    
    Parameters
    ----------
    article_url : str
        æ–‡ç«  URL
    articles_info : ArticlesInfo, optional
        ArticlesInfo å®ä¾‹ï¼ˆç”¨äº API æ–¹å¼ï¼‰
    use_html_extraction : bool
        æ˜¯å¦ä½¿ç”¨ HTML æå–æ–¹å¼ï¼ˆæ¨èï¼Œæ›´ç¨³å®šï¼‰
    
    Returns
    -------
    dict
        ç»Ÿè®¡æ•°æ®
    """
    # ä¼˜å…ˆä½¿ç”¨ HTML æå–æ–¹å¼
    if use_html_extraction:
        try:
            from extract_stats_from_html import get_article_stats_from_url
            from params.new_wechat_config import KEY, UIN, PASS_TICKET
            
            # ä» HTML æå–ç»Ÿè®¡æ•°æ®ï¼ˆä¼ å…¥å¿…è¦çš„å‚æ•°ä»¥è·å–å®Œæ•´HTMLï¼‰
            stats = get_article_stats_from_url(
                article_url, 
                cookie=COOKIE,
                key=KEY,
                uin=UIN,
                pass_ticket=PASS_TICKET
            )
            
            if stats.get('success'):
                return {
                    'read_count': stats.get('read_num', 0),
                    'old_like_count': stats.get('old_like_count', 0),  # ç‚¹èµï¼ˆå¤§æ‹‡æŒ‡ğŸ‘ï¼‰
                    'like_count': stats.get('like_count', 0),  # å–œæ¬¢/æ”¶è—ï¼ˆçˆ±å¿ƒâ¤ï¸ï¼‰
                    'comment_count': stats.get('comment_count', 0),
                    'share_count': stats.get('share_count', 0),
                    'nickname': stats.get('nickname', ''),
                    'user_name': stats.get('user_name', ''),
                    'createTime': stats.get('createTime', ''),
                    'article_type': stats.get('article_type', 'å›¾æ–‡'),
                    'success': True,
                    'method': 'html_extraction'
                }
            else:
                # HTML æå–å¤±è´¥ï¼Œå°è¯• API æ–¹å¼
                print(f"      âš ï¸  HTML æå–å¤±è´¥ï¼Œå°è¯• API æ–¹å¼...")
                if articles_info:
                    return get_article_stats_api(article_url, articles_info)
                else:
                    return {
                        'read_count': 'N/A',
                        'like_count': 'N/A',
                        'comment_count': 'N/A',
                        'share_count': 'N/A',
                        'success': False,
                        'error': 'HTML extraction failed and no API instance provided'
                    }
        except Exception as e:
            print(f"      âš ï¸  HTML æå–å‡ºé”™: {e}")
            # å¦‚æœæœ‰ API å®ä¾‹ï¼Œå°è¯•ä½¿ç”¨ API æ–¹å¼
            if articles_info:
                print(f"      å°è¯•ä½¿ç”¨ API æ–¹å¼...")
                return get_article_stats_api(article_url, articles_info)
            else:
                return {
                    'read_count': 'N/A',
                    'like_count': 'N/A',
                    'comment_count': 'N/A',
                    'share_count': 'N/A',
                    'success': False,
                    'error': str(e)
                }
    else:
        # ä½¿ç”¨ API æ–¹å¼
        if articles_info:
            return get_article_stats_api(article_url, articles_info)
        else:
            return {
                'read_count': 'N/A',
                'like_count': 'N/A',
                'comment_count': 'N/A',
                'share_count': 'N/A',
                'success': False,
                'error': 'No API instance provided'
            }


def get_article_stats_api(article_url, articles_info):
    """
    ä½¿ç”¨ API æ–¹å¼è·å–æ–‡ç« ç»Ÿè®¡æ•°æ®ï¼ˆå¤‡ç”¨æ–¹å¼ï¼‰
    
    Parameters
    ----------
    article_url : str
        æ–‡ç«  URL
    articles_info : ArticlesInfo
        ArticlesInfo å®ä¾‹
    
    Returns
    -------
    dict
        ç»Ÿè®¡æ•°æ®
    """
    try:
        # è·å–é˜…è¯»æ•°å’Œç‚¹èµæ•°
        read_num, like_num, old_like_num = articles_info.read_like_nums(article_url)
        
        # è·å–è¯„è®ºæ•°
        comments_data = articles_info.comments(article_url)
        comment_count = comments_data.get('elected_comment_total_cnt', 0) if comments_data else 0
        
        return {
            'read_count': read_num,
            'like_count': like_num,
            'old_like_count': old_like_num,
            'comment_count': comment_count,
            'share_count': 0,  # API æ–¹å¼æ— æ³•è·å–åˆ†äº«æ•°
            'success': True,
            'method': 'api'
        }
    except Exception as e:
        return {
            'read_count': 'N/A',
            'like_count': 'N/A',
            'old_like_count': 'N/A',
            'comment_count': 'N/A',
            'share_count': 'N/A',
            'success': False,
            'error': str(e),
            'method': 'api'
        }


def parse_date_range(date_str):
    """è§£ææ—¥æœŸèŒƒå›´"""
    if not date_str:
        return None, None
    
    # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
    for sep in [' åˆ° ', '~', ',']:
        if sep in date_str:
            parts = date_str.split(sep)
            if len(parts) == 2:
                try:
                    start_date = datetime.strptime(parts[0].strip(), '%Y-%m-%d')
                    end_date = datetime.strptime(parts[1].strip(), '%Y-%m-%d')
                    return start_date, end_date
                except:
                    continue
    
    # å•ä¸ªæ—¥æœŸ
    try:
        date = datetime.strptime(date_str.strip(), '%Y-%m-%d')
        return date, date
    except:
        return None, None


def save_to_csv(data, filename):
    """ä¿å­˜æ•°æ®åˆ° CSV"""
    if not data:
        return
    
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        fieldnames = [
            'title', 'url', 'local_html_path', 'publish_date',
            'nickname', 'user_name', 'article_type',
            'read_count', 'old_like_count', 'share_count', 'like_count', 'comment_count',
            'digest'
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        writer.writerows(data)
    
    print(f"âœ… CSV å·²ä¿å­˜: {filename}")




def save_to_json(data, filename):
    """ä¿å­˜æ•°æ®åˆ° JSON"""
    if not data:
        return
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSON å·²ä¿å­˜: {filename}")


def main():
    """ä¸»å‡½æ•°"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘          æ™ºèƒ½æ‰¹é‡è·å–å…¬ä¼—å·æ–‡ç« ä¿¡æ¯å·¥å…·                   â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # è¾“å…¥æ–‡ç«  URL
    print("ğŸ“ è¯·è¾“å…¥å…¬ä¼—å·çš„ä»»æ„ä¸€ç¯‡æ–‡ç«  URL:\n")
    article_url = input("æ–‡ç«  URL: ").strip()
    
    if not article_url:
        print("âŒ URL ä¸èƒ½ä¸ºç©º")
        return
    
    # æå– BIZ
    print(f"\nğŸ” æ­£åœ¨æå– BIZ...")
    biz = extract_biz_from_url(article_url)
    
    if not biz:
        print("âŒ æ— æ³•ä» URL ä¸­æå– BIZ")
        print("ğŸ’¡ è¯·ç¡®ä¿ URL æ ¼å¼æ­£ç¡®ï¼Œä¾‹å¦‚:")
        print("   https://mp.weixin.qq.com/s?__biz=xxx&mid=xxx...")
        return
    
    print(f"âœ… BIZ: {biz}")
    
    # è¾“å…¥æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰
    print(f"\nğŸ“… è¯·è¾“å…¥æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼Œç›´æ¥å›è½¦è·å–æ‰€æœ‰æ–‡ç« ï¼‰:")
    print(f"   æ ¼å¼: 2024-12-01 åˆ° 2024-12-10\n")
    
    date_range_str = input("æ—¥æœŸèŒƒå›´: ").strip()
    start_date, end_date = parse_date_range(date_range_str)
    
    if date_range_str and (not start_date or not end_date):
        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
        return
    
    # è·å–æ–‡ç« åˆ—è¡¨
    articles = fetch_articles_from_profile(biz, start_date, end_date)
    
    if not articles:
        print("\nâš ï¸  æ²¡æœ‰è·å–åˆ°æ–‡ç« ")
        return
    
    # æå– appmsg_token
    appmsg_token = extract_appmsg_token_from_cookie(COOKIE)
    if not appmsg_token:
        print("âŒ æ— æ³•ä» Cookie ä¸­æå– appmsg_token")
        return
    
    # åˆ›å»º ArticlesInfo å®ä¾‹
    articles_info = ArticlesInfo(appmsg_token, COOKIE)
    
    # æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®
    print(f"ğŸ“Š å¼€å§‹æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®...\n")
    
    results = []
    success_count = 0
    
    for i, article in enumerate(articles, 1):
        print(f"[{i}/{len(articles)}] {article['title']}")
        print(f"   å‘å¸ƒæ—¶é—´: {article['publish_date']}")
        
        # âœ… ä¼˜å…ˆä½¿ç”¨ä¸‹è½½æ—¶å·²æå–çš„ç»Ÿè®¡æ•°æ®ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
        
        if article.get('_extracted_stats') and isinstance(article.get('_extracted_stats'), dict) and len(article['_extracted_stats']) > 0:
            print(f"   âœ… ä½¿ç”¨ä¸‹è½½æ—¶æå–çš„ç»Ÿè®¡æ•°æ®ï¼ˆæ— éœ€é‡å¤è¯·æ±‚ï¼‰")
            extracted = article['_extracted_stats']
            stats = {
                'read_count': int(extracted.get('read_num', 0)) if extracted.get('read_num') else 0,
                'old_like_count': int(extracted.get('old_like_count', 0)) if extracted.get('old_like_count') else 0,  # ç‚¹èµï¼ˆå¤§æ‹‡æŒ‡ğŸ‘ï¼‰
                'like_count': int(extracted.get('like_count', 0)) if extracted.get('like_count') else 0,  # å–œæ¬¢/æ”¶è—ï¼ˆçˆ±å¿ƒâ¤ï¸ï¼‰
                'comment_count': int(extracted.get('comment_count', 0)) if extracted.get('comment_count') else 0,
                'share_count': int(extracted.get('share_count', 0)) if extracted.get('share_count') else 0,
                'nickname': extracted.get('nickname', ''),
                'user_name': extracted.get('user_name', ''),
                'article_type': extracted.get('article_type', 'å›¾æ–‡'),
                'success': True,
                'method': 'from_download_cache'
            }
        else:
            # å¦‚æœä¸‹è½½æ—¶æ²¡æœ‰æå–åˆ°ï¼Œå†è¯·æ±‚ä¸€æ¬¡
            print(f"   æ­£åœ¨è·å–ç»Ÿè®¡æ•°æ®...")
            stats = get_article_stats(article['url'], articles_info)
        
        if stats['success']:
            success_count += 1
            share_info = f" | åˆ†äº«: {stats['share_count']}" if stats.get('share_count', 0) > 0 else ""
            method_info = f" [{stats.get('method', 'unknown')}]" if 'method' in stats else ""
            # âœ… è¾“å‡ºé¡ºåºï¼šé˜…è¯»é‡ã€ç‚¹èµã€åˆ†äº«ã€å–œæ¬¢ã€è¯„è®º
            print(f"   âœ… é˜…è¯»: {stats['read_count']:,} | ç‚¹èµ: {stats.get('old_like_count', 0):,} | åˆ†äº«: {stats['share_count']:,} | å–œæ¬¢: {stats.get('like_count', 0):,} | è¯„è®º: {stats['comment_count']}{method_info}")
        else:
            print(f"   âŒ è·å–å¤±è´¥{': ' + stats.get('error', '') if stats.get('error') else ''}")
        
        # åˆå¹¶æ•°æ®
        result = {**article, **stats}
        results.append(result)
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if i < len(articles):
            time.sleep(2)
        
        print()
    
    print(f"{'='*80}")
    print(f"âœ… æ‰¹é‡è·å–å®Œæˆ")
    print(f"{'='*80}")
    print(f"æ€»å…±: {len(articles)} ç¯‡æ–‡ç« ")
    print(f"æˆåŠŸ: {success_count} ç¯‡")
    print(f"å¤±è´¥: {len(articles) - success_count} ç¯‡")
    print(f"{'='*80}\n")
    
    # ä¿å­˜æ•°æ®
    print("ğŸ’¾ ä¿å­˜æ•°æ®...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    csv_filename = f"articles_{timestamp}.csv"
    json_filename = f"articles_{timestamp}.json"
    
    save_to_csv(results, csv_filename)
    save_to_json(results, json_filename)
    
    print(f"\nğŸ‰ å®Œæˆï¼å…±è·å– {len(results)} ç¯‡æ–‡ç« çš„æ•°æ®")


if __name__ == "__main__":
    main()
