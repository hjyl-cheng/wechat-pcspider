# coding: utf-8
"""
æ–°çš„APIç«¯ç‚¹å®ç°ï¼ˆä½¿ç”¨æ•°æ®åº“ç¼“å­˜ï¼‰
è¿™ä¸ªæ–‡ä»¶åŒ…å«é‡æ„åçš„APIç«¯ç‚¹ï¼Œå°†é€æ­¥æ›¿æ¢api_server.pyä¸­çš„æ—§å®ç°
"""
from flask import request, jsonify
from datetime import datetime, timedelta
import logging
import re
import json
import requests
import time
import os
# å¯¼å…¥æ•°æ®åº“æ“ä½œ
from db_operations import (
    get_or_create_account,
    save_parameters,
    get_valid_parameters,
    invalidate_parameters,
    save_article,
    get_article,
    get_articles_by_filters,
    is_article_fresh
)
from db_helpers import get_biz_by_account_name
# å¯¼å…¥ç°æœ‰åŠŸèƒ½
from smart_batch_fetch import (
    extract_appmsg_token_from_cookie,
    extract_biz_from_url,
)
from download_full_html import download_full_html_with_stats
from wechatarticles import ArticlesInfo
logger = logging.getLogger(__name__)


def _write_params_to_config(params):
    """
    å°†æ•°æ®åº“å‚æ•°å†™å…¥params/new_wechat_config.py
    è¿™æ ·download_full_html.pyå°±èƒ½è¯»å–åˆ°æ­£ç¡®çš„å‚æ•°
    
    Parameters
    ----------
    params : dict
        æ•°æ®åº“ä¸­çš„å‚æ•°ï¼ŒåŒ…å« cookie, key, pass_ticket, uin ç­‰
    """
    from datetime import datetime
    
    # æ•°æ®åº“å‚æ•°æ˜¯å°å†™çš„ï¼Œéœ€è¦è½¬æ¢ä¸ºå¤§å†™
    config_content = f'''# coding: utf-8
# ç”±api_endpoints_new.pyè‡ªåŠ¨ç”Ÿæˆ
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

COOKIE = '{params.get('cookie', '')}'

KEY = '{params.get('key', '')}'

PASS_TICKET = '{params.get('pass_ticket', '')}'

UIN = '{params.get('uin', '')}'

DEVICETYPE = 'UnifiedPCWindows'

CLIENTVERSION = ''

BIZ = ''
'''
    
    with open('params/new_wechat_config.py', 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    # é‡æ–°åŠ è½½æ¨¡å—ï¼Œç¡®ä¿ä¸‹æ¬¡å¯¼å…¥æ—¶ä½¿ç”¨æ–°å‚æ•°
    import importlib
    import params.new_wechat_config
    importlib.reload(params.new_wechat_config)
    
    logger.info(f"   âœ… å·²æ›´æ–°params/new_wechat_config.py")
def fetch_articles_with_params(biz, params, start_date=None, end_date=None, should_stop_func=None):
    """
    ä½¿ç”¨æ•°æ®åº“å‚æ•°è·å–å…¬ä¼—å·æ–‡ç« åˆ—è¡¨ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    
    Parameters
    ----------
    biz : str
        å…¬ä¼—å·BIZ
    params : dict
        æ•°æ®åº“ä¸­çš„å‚æ•°
    start_date : datetime, optional
        å¼€å§‹æ—¥æœŸ
    end_date : datetime, optional
        ç»“æŸæ—¥æœŸ
    should_stop_func : function, optional
        å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ article å­—å…¸ï¼Œè¿”å› True åˆ™åœæ­¢è·å–
    
    Returns
    -------
    list
        æ–‡ç« åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ ä½¿ç”¨æ•°æ®åº“å‚æ•°è·å–æ–‡ç« åˆ—è¡¨...")
    
    # æ„é€ è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 MicroMessenger/3.4.0',
        'Cookie': params['cookie'],
        'Referer': f'https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz={biz}&scene=124',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'X-Requested-With': 'XMLHttpRequest',
    }
    
    all_articles = []
    offset = 0
    count = 10
    has_more = True
    
    while has_more:
        # ... (API URL construction) ...
        api_url = (
            f"https://mp.weixin.qq.com/mp/profile_ext?"
            f"action=getmsg&"
            f"__biz={biz}&"
            f"f=json&"
            f"offset={offset}&"
            f"count={count}&"
            f"is_ok=1&"
            f"scene=124&"
            f"uin={params.get('uin', '')}&"
            f"key={params.get('key', '')}&"
            f"pass_ticket={params.get('pass_ticket', '')}&"
            f"wxtoken=&"
            f"appmsg_token={params.get('appmsg_token', '')}&"
            f"x5=0"
        )
        
        try:
            logger.info(f"   è·å–ç¬¬ {offset//count + 1} é¡µ...")
            
            response = requests.get(api_url, headers=headers, timeout=10)
            data = response.json()
            
            if data.get('ret') != 0:
                # Error handling...
                errmsg = data.get('errmsg', 'Unknown')
                if 'no session' in errmsg.lower():
                    return {'error': 'no_session', 'message': 'å‚æ•°å·²å¤±æ•ˆ'}
                logger.warning(f"   âš ï¸  APIè¿”å›é”™è¯¯: {errmsg}")
                break
            
            # è§£ææ–‡ç« åˆ—è¡¨
            general_msg_list = data.get('general_msg_list', {})
            if isinstance(general_msg_list, str):
                general_msg_list = json.loads(general_msg_list)
            
            msg_list = general_msg_list.get('list', [])
            
            if not msg_list:
                logger.info(f"   âœ… å·²è·å–æ‰€æœ‰æ–‡ç« ")
                break
            
            # å¤„ç†æ¯æ¡æ¶ˆæ¯
            for msg in msg_list:
                comm_msg_info = msg.get('comm_msg_info', {})
                app_msg_ext_info = msg.get('app_msg_ext_info', {})
                
                if not app_msg_ext_info:
                    continue
                
                publish_time = comm_msg_info.get('datetime', 0)
                article_date = datetime.fromtimestamp(publish_time)
                
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                if end_date and article_date > end_date + timedelta(days=1):
                    # æ–‡ç« å¤ªæ–°ï¼Œè·³è¿‡ï¼Œç»§ç»­æ‰¾æ—§çš„
                    continue
                
                if start_date and article_date < start_date:
                    # æ–‡ç« å¤ªæ—§ï¼Œç»“æŸè·å–
                    logger.info(f"   ğŸ›‘ é‡åˆ°æ—©äºå¼€å§‹æ—¥æœŸçš„æ–‡ç«  ({article_date.date()})ï¼Œåœæ­¢è·å–")
                    has_more = False
                    break
                
                # ä¸»æ–‡ç« 
                article_url = app_msg_ext_info.get('content_url', '').replace('\\/', '/')
                article_title = app_msg_ext_info.get('title', '')
                
                article = {
                    'title': article_title,
                    'url': article_url,
                    'digest': app_msg_ext_info.get('digest', ''),
                    'cover': app_msg_ext_info.get('cover', ''),
                    'publish_time': publish_time,
                    'publish_date': article_date.strftime('%Y-%m-%d'),
                }
                
                # æ£€æŸ¥ä¸»æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
                main_article_exists = should_stop_func and should_stop_func(article) if should_stop_func else False
                
                if article['url'] and not main_article_exists:
                    all_articles.append(article)
                
                # å¤šå›¾æ–‡æ¶ˆæ¯ï¼ˆå³ä½¿ä¸»æ–‡ç« å­˜åœ¨ï¼Œä¹Ÿè¦å¤„ç†å¤šå›¾æ–‡ï¼‰
                multi_app_msg_item_list = app_msg_ext_info.get('multi_app_msg_item_list', [])
                if multi_app_msg_item_list:
                    logger.info(f"   ğŸ“‘ å‘ç° {len(multi_app_msg_item_list)} ç¯‡å¤šå›¾æ–‡")
                for item in multi_app_msg_item_list:
                    sub_article = {
                        'title': item.get('title', ''),
                        'url': item.get('content_url', '').replace('\\/', '/'),
                        'digest': item.get('digest', ''),
                        'cover': item.get('cover', ''),
                        'publish_time': publish_time,
                        'publish_date': article_date.strftime('%Y-%m-%d'),
                    }
                    if sub_article['url']:
                        # æ£€æŸ¥å­æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
                        sub_exists = should_stop_func and should_stop_func(sub_article) if should_stop_func else False
                        if not sub_exists:
                            all_articles.append(sub_article)
                
                # å¦‚æœä¸»æ–‡ç« å·²å­˜åœ¨ï¼Œä¸”æ‰€æœ‰å­æ–‡ç« ä¹Ÿéƒ½å­˜åœ¨ï¼Œåˆ™åœæ­¢è·å–
                if main_article_exists:
                    logger.info(f"   ğŸ›‘ é‡åˆ°å·²å­˜åœ¨çš„æ–‡ç« ï¼Œåœæ­¢è·å–: {article_title}")
                    has_more = False
                    break
            
            offset += count
            time.sleep(1 + (offset % 3)) # éšæœºå»¶è¿Ÿ
            
        except Exception as e:
            logger.error(f"   âŒ è·å–å¤±è´¥: {e}")
            break
    
    logger.info(f"âœ… ä»APIæ–°è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
    
    # è°ƒè¯•ï¼šæ‰“å°æ¯ç¯‡æ–‡ç« çš„URL
    for idx, art in enumerate(all_articles, 1):
        logger.debug(f"  [{idx}] {art.get('title', '')[:40]}: {art.get('url', '')[:100]}")
    
    return all_articles
def fetch_article_with_cache():
    """
    è·å–å•ç¯‡æ–‡ç« æ•°æ®ï¼ˆä½¿ç”¨æ•°æ®åº“ç¼“å­˜ï¼‰
    
    æµç¨‹ï¼š
    1. æå–BIZ
    2. æ£€æŸ¥æ•°æ®åº“ç¼“å­˜
    3. å¦‚æœç¼“å­˜æ–°é²œï¼ˆ<24å°æ—¶ï¼‰â†’ è¿”å›ç¼“å­˜
    4. å¦åˆ™ â†’ æ£€æŸ¥å‚æ•° â†’ è°ƒç”¨å¾®ä¿¡API â†’ å­˜å‚¨åˆ°æ•°æ®åº“ â†’ è¿”å›
    """
    try:
        # è§£æè¯·æ±‚
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'}), 400
        
        account_name = data.get('account_name')
        article_url = data.get('article_url')
        
        if not article_url:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°: article_url'}), 400
        
        logger.info(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚: å…¬ä¼—å·={account_name}, URL={article_url}")
        
        # 1. è·å–BIZå’Œå‚æ•°ï¼ˆä¼˜å…ˆä»æ•°æ®åº“ï¼‰
        biz = None
        params = None
        
        # æ–¹æ³•1ï¼šå¦‚æœæä¾›äº†account_nameï¼Œä»æ•°æ®åº“æŸ¥è¯¢BIZå’Œå‚æ•°
        if account_name:
            biz, params = get_biz_by_account_name(account_name)
            if biz:
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–BIZ: {biz} (è´¦å·: {account_name})")
                if params:
                    logger.info(f"   åŒæ—¶è·å–åˆ°å‚æ•° (Cookieé•¿åº¦: {len(params.get('cookie', ''))})")
        
        # æ–¹æ³•2ï¼šä»URLæå–BIZï¼ˆå¦‚æœæ–¹æ³•1æ²¡è·å–åˆ°ï¼‰
        if not biz:
            logger.info(f"   ä»URLæå–BIZ...")
            biz = extract_biz_from_url(article_url)
            if not biz:
                error_msg = 'æ— æ³•ä»URLæå–BIZ'
                if 'wappoc_appmsgcaptcha' in article_url or 'captcha' in article_url.lower():
                    error_msg = 'è¯¥æ–‡ç« éœ€è¦éªŒè¯ç ï¼Œè¯·å°è¯•ï¼š1) æ›´æ¢å…¶ä»–æ–‡ç« URLï¼Œ2) åœ¨å¾®ä¿¡PCç«¯æ‰‹åŠ¨æ‰“å¼€æ–‡ç« åé‡è¯•'
                else:
                    error_msg = 'æ— æ³•ä»URLæå–BIZï¼Œè¯·æ£€æŸ¥URLæ ¼å¼æˆ–å°è¯•å…¶ä»–æ–‡ç« '
                
                logger.error(f"âŒ {error_msg}")
                return jsonify({'success': False, 'error': error_msg}), 400
            
            logger.info(f"âœ… ä»URLæå–BIZ: {biz}")
            
            # æå–åˆ°BIZåï¼Œå°è¯•è·å–å‚æ•°
            if not params:
                params = get_valid_parameters(biz)
                if params:
                    logger.info(f"   è·å–åˆ°å·²å­˜å‚¨çš„å‚æ•°")
        
        # æ›´æ–°è´¦å·ä¿¡æ¯
        get_or_create_account(biz, account_name)
        
        # 2. æ£€æŸ¥æ–‡ç« æ•°æ®ç¼“å­˜
        cached_article = get_article(article_url)
        if cached_article and is_article_fresh(article_url, max_age_hours=24):
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ–‡ç« æ•°æ®: {cached_article.get('title')}")
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': True,
                    **cached_article
                }
            })
        
        # 3. æ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦ä»å¾®ä¿¡APIè·å–
        logger.info(f"ğŸ“¡ æ–‡ç« æ•°æ®æœªç¼“å­˜ï¼Œä»å¾®ä¿¡APIè·å–...")
        
        # 3.1 ç¡®ä¿æœ‰å‚æ•°
        if not params:
            # å‚æ•°ä¸å­˜åœ¨ï¼Œéœ€è¦æ•è·
            logger.info(f"âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å‚æ•°ï¼Œå¼€å§‹è‡ªåŠ¨æ•è·...")
            
            from api_server import ProxyManager
            if not ProxyManager.start_proxy_and_capture(article_url, biz=biz, timeout=120):
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·å¤±è´¥ï¼Œè¯·ç¡®ä¿å¾®ä¿¡å·²æ­£å¸¸è¿è¡Œ'
                }), 500
            
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ•°æ®åº“æ›´æ–°
            time.sleep(2)
            
            # é‡æ–°è·å–å‚æ•°
            params = get_valid_parameters(biz)
            if not params:
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·åä»æ— æ³•ä»æ•°æ®åº“è·å–'
                }), 500
        
        logger.info(f"âœ… ä½¿ç”¨å‚æ•° (Cookieé•¿åº¦: {len(params.get('cookie', ''))})")
        
        # 3.2 è½¬æ¢çŸ­é“¾æ¥ä¸ºé•¿é“¾æ¥
        final_article_url = article_url
        if '/s/' in article_url and '__biz=' not in article_url:
            logger.info(f"   æ£€æµ‹åˆ°çŸ­é“¾æ¥ï¼Œæ­£åœ¨è½¬æ¢...")
            try:
                import requests
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Cookie': params['cookie'],
                }
                response = requests.get(article_url, headers=headers, timeout=15)
                
                # ä»é¡µé¢å†…å®¹æå–å®Œæ•´URL
                patterns = [
                    r'var\s+msg_link\s*=\s*["\']([^"\']+)["\']',
                    r'url:\s*["\']([^"\']+/s\?[^"\']+)["\']',
                    r'window.msg_link = "([^"]+)"',
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, response.text)
                    if match:
                        extracted_url = match.group(1)
                        # è½¬æ¢HTMLå®ä½“
                        extracted_url = extracted_url.replace('&amp;', '&').replace('\\/', '/')
                        if '__biz=' in extracted_url and 'mid=' in extracted_url:
                            final_article_url = extracted_url
                            logger.info(f"   âœ… è½¬æ¢æˆåŠŸ: {final_article_url[:80]}...")
                            break
                else:
                    logger.warning(f"   âš ï¸  æ— æ³•ä»é¡µé¢æå–å®Œæ•´URL")
                    
            except Exception as e:
                logger.warning(f"   âš ï¸  è½¬æ¢å¤±è´¥: {e}")
        
        # 3.3 è°ƒç”¨å¾®ä¿¡APIè·å–æ•°æ®
        try:
            articles_info = ArticlesInfo(
                appmsg_token=params['appmsg_token'],
                cookie=params['cookie']
            )
            stats = get_article_stats(final_article_url, articles_info)
            
            if not stats or not stats.get('success'):
                error_msg = stats.get('error', 'æœªçŸ¥é”™è¯¯') if stats else 'è¿”å›å€¼ä¸ºç©º'
                
                # è®°å½•é”™è¯¯ä½†ä¸è‡ªåŠ¨é‡æ–°æ•è·ï¼ˆé¿å…é¢‘ç¹æ•è·ï¼‰
                logger.error(f"âŒ å¾®ä¿¡APIè¿”å›é”™è¯¯: {error_msg}")
                
                # å¦‚æœæ˜¯å‚æ•°é”™è¯¯ï¼Œæç¤ºç”¨æˆ·å¯èƒ½éœ€è¦é‡æ–°æ•è·
                if 'params is error' in error_msg or 'no session' in error_msg:
                    # æ³¨æ„ï¼šä¸ç«‹å³æ ‡è®°å¤±æ•ˆï¼Œè®©ç”¨æˆ·å†³å®š
                    logger.warning(f"âš ï¸  å¯èƒ½å‚æ•°å·²å¤±æ•ˆï¼Œå»ºè®®é‡æ–°æ•è·")
                    return jsonify({
                        'success': False,
                        'error': f'å‚æ•°å¯èƒ½å·²å¤±æ•ˆ: {error_msg}',
                        'need_recapture': True,
                        'biz': biz
                    }), 400
                else:
                    # å…¶ä»–é”™è¯¯
                    return jsonify({
                        'success': False,
                        'error': f'è·å–æ–‡ç« æ•°æ®å¤±è´¥: {error_msg}'
                    }), 500
            
            # 3.4 è·å–æ–‡ç« æ ‡é¢˜å’ŒHTMLå†…å®¹
            logger.info(f"   æ­£åœ¨è·å–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹...")
            article_title = None
            article_html = None
            try:
                import requests as req
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Cookie': params['cookie'],
                }
                article_response = req.get(final_article_url, headers=headers, timeout=15)
                article_html = article_response.text
                
                # ä»HTMLæå–æ ‡é¢˜
                title_match = re.search(r'<h1[^>]*class="rich_media_title"[^>]*>([^<]+)</h1>', article_html)
                if title_match:
                    article_title = title_match.group(1).strip()
                else:
                    # å°è¯•å…¶ä»–æ–¹å¼
                    title_match2 = re.search(r'<meta\s+property="og:title"\s+content="([^"]+)"', article_html)
                    if title_match2:
                        article_title = title_match2.group(1).strip()
                
                logger.info(f"   âœ… è·å–åˆ°æ ‡é¢˜: {article_title}")
            except Exception as e:
                logger.warning(f"   âš ï¸  è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            
            # 3.5 ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåŒ…å«å®Œæ•´æ•°æ®ï¼‰
            # æ¸…ç†ç»Ÿè®¡æ•°æ®ï¼šå°† "N/A" æˆ–éæ•°å­—å€¼è½¬æ¢ä¸º None
            def clean_stat(value):
                if value == "N/A" or value is None:
                    return None
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return None
            
            article_data = {
                'biz': biz,
                'url': final_article_url,  # ä¿å­˜å®Œæ•´URL
                'short_url': article_url,   # ä¿å­˜çŸ­é“¾æ¥ä¾¿äºæŸ¥æ‰¾
                'title': article_title,
                'html_content': article_html,
                'publish_date': None,  # æš‚ä¸æå–
                'read_count': clean_stat(stats.get('read_count')),
                'like_count': clean_stat(stats.get('like_count')),
                'comment_count': clean_stat(stats.get('comment_count'))
            }
            
            saved_article = save_article(article_data)
            
            logger.info(f"âœ… æˆåŠŸè·å–å¹¶ä¿å­˜æ–‡ç« æ•°æ®: {saved_article.get('title')}")
            
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': False,
                    **saved_article
                }
            })
            
        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨å¾®ä¿¡APIå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({
                'success': False,
                'error': f'è°ƒç”¨å¾®ä¿¡APIå¤±è´¥: {str(e)}'
            }), 500
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500
def fetch_articles_filtered():
    """
    æ‰¹é‡è·å–æ–‡ç« ï¼ˆå¸¦è¿‡æ»¤ï¼‰
    
    è¯·æ±‚ä½“ï¼š
    {
        "account_name": "å…¬ä¼—å·åç§°",
        "article_url": "ä»»æ„ä¸€ç¯‡æ–‡ç« URLï¼ˆç”¨äºæå–BIZï¼‰",
        "start_date": "2024-12-01",
        "end_date": "2024-12-10",
        "min_read_count": 10000,
        "limit": 10  // å¯é€‰ï¼Œé™åˆ¶æ•°é‡
    }
    """
    try:
        # è§£æè¯·æ±‚
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'}), 400
        
        account_name = data.get('account_name')
        article_url = data.get('article_url')
        start_date_str = data.get('start_date')
        end_date_str = data.get('end_date')
        min_read_count = data.get('min_read_count')
        limit = data.get('limit', 20)  # é»˜è®¤é™åˆ¶20ç¯‡
        
        if not article_url:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°: article_url'}), 400
        
        logger.info(f"ğŸ“¥ æ”¶åˆ°æ‰¹é‡è¯·æ±‚: å…¬ä¼—å·={account_name}")
        
        # 1. ä¼˜å…ˆä»URLæå–BIZï¼ˆURLä¸­çš„BIZæ˜¯æœ€å‡†ç¡®çš„ï¼‰
        biz = extract_biz_from_url(article_url)
        if not biz:
            return jsonify({'success': False, 'error': 'æ— æ³•ä»URLæå–BIZ'}), 400
        logger.info(f"âœ… ä»URLæå–BIZ: {biz}")
        
        # 2. è·å–è¯¥BIZçš„å‚æ•°
        params = get_valid_parameters(biz)
        
        # åˆ›å»ºæˆ–æ›´æ–°è´¦å·
        get_or_create_account(biz, account_name)
        
        # 2. ç¡®ä¿æœ‰å‚æ•°ï¼ˆè‡ªåŠ¨æ•è·ï¼‰
        if not params:
            logger.info(f"âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å‚æ•°ï¼Œå¼€å§‹è‡ªåŠ¨æ•è·...")
            
            from api_server import ProxyManager
            if not ProxyManager.start_proxy_and_capture(article_url, biz=biz, timeout=120):
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·å¤±è´¥ï¼Œè¯·ç¡®ä¿å¾®ä¿¡å·²æ­£å¸¸è¿è¡Œ'
                }), 500
            
            time.sleep(2)
            
            params = get_valid_parameters(biz)
            if not params:
                return jsonify({
                    'success': False,
                    'error': 'å‚æ•°æ•è·åä»æ— æ³•ä»æ•°æ®åº“è·å–'
                }), 500
        
        # 3. è§£ææ—¥æœŸ
        start_date = datetime.strptime(start_date_str, '%Y-%m-%d') if start_date_str else None
        end_date = datetime.strptime(end_date_str, '%Y-%m-%d') if end_date_str else None
        
        # 4. æ£€æŸ¥æ•°æ®åº“ä¸­è¯¥æ—¥æœŸèŒƒå›´å†…æ¯ä¸€å¤©æ˜¯å¦éƒ½æœ‰æ•°æ®
        from database import get_db_session
        from models import Article
        
        missing_dates = []
        if start_date and end_date:
            with get_db_session() as session:
                # è·å–è¯¥BIZåœ¨æ—¥æœŸèŒƒå›´å†…æ‰€æœ‰æ–‡ç« çš„å‘å¸ƒæ—¥æœŸ
                existing_articles = session.query(Article.publish_date).filter(
                    Article.biz == biz,
                    Article.publish_date >= start_date.strftime('%Y-%m-%d'),
                    Article.publish_date <= end_date.strftime('%Y-%m-%d')
                ).all()
                
                existing_dates = set()
                for row in existing_articles:
                    if row.publish_date:
                        # publish_dateå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–datetime
                        if isinstance(row.publish_date, str):
                            existing_dates.add(row.publish_date)
                        else:
                            existing_dates.add(row.publish_date.strftime('%Y-%m-%d'))
                
                # æ£€æŸ¥æ¯ä¸€å¤©æ˜¯å¦éƒ½æœ‰æ•°æ®
                current_date = start_date
                while current_date <= end_date:
                    date_str = current_date.strftime('%Y-%m-%d')
                    if date_str not in existing_dates:
                        missing_dates.append(date_str)
                    current_date += timedelta(days=1)
        
        if not missing_dates:
            # æ‰€æœ‰æ—¥æœŸéƒ½æœ‰æ•°æ®ï¼Œç›´æ¥ä»æ•°æ®åº“è¿”å›
            logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰å®Œæ•´æ•°æ®ï¼ˆ{start_date_str} ~ {end_date_str}ï¼‰ï¼Œç›´æ¥è¿”å›")
            db_articles = get_articles_by_filters(biz, start_date, end_date, min_read_count)
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': True,
                    'total_saved': 0,
                    'total': len(db_articles[:limit]),
                    'articles': db_articles[:limit]
                }
            })
        
        # 5. æœ‰ç¼ºå¤±æ—¥æœŸï¼Œéœ€è¦ä»APIè·å–
        logger.info(f"ğŸ“¡ æ•°æ®åº“ç¼ºå¤±ä»¥ä¸‹æ—¥æœŸçš„æ•°æ®: {', '.join(missing_dates)}")
        logger.info(f"   å¼€å§‹ä»å¾®ä¿¡APIè·å–ç¼ºå¤±æ•°æ®...")
        
        # è·å–ç°æœ‰æ–‡ç« æŒ‡çº¹ï¼ˆç”¨äºå¢é‡æ›´æ–°åˆ¤æ–­ï¼‰
        existing_titles = set()
        
        with get_db_session() as session:
            existing = session.query(Article.title).filter(Article.biz == biz).all()
            for row in existing:
                if row.title:
                    existing_titles.add(row.title)
        
        logger.info(f"ğŸ“š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_titles)} ç¯‡æ–‡ç« ï¼ˆå…¨éƒ¨å†å²ï¼‰")
        
        # å®šä¹‰åœæ­¢æŠ“å–çš„å›è°ƒå‡½æ•°
        def should_stop_fetch(article):
            # å¦‚æœæ ‡é¢˜å·²å­˜åœ¨ï¼Œè¯´æ˜æ¥ä¸Šå†å²æ•°æ®äº†
            title = article.get('title', '')
            if title and title in existing_titles:
                return True
            return False
        # 5. ä»å¾®ä¿¡APIè·å–ï¼ˆå¢é‡æ¨¡å¼ï¼‰
        logger.info(f"ğŸ“¡ ä»å¾®ä¿¡APIè·å–æ–‡ç« ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
        
        try:
            articles_info = ArticlesInfo(
                appmsg_token=params['appmsg_token'],
                cookie=params['cookie']
            )
            
            # è·å–æ–‡ç« åˆ—è¡¨ï¼ˆä½¿ç”¨æ•°æ®åº“å‚æ•°ï¼Œä¼ å…¥å›è°ƒï¼‰
            articles = fetch_articles_with_params(biz, params, start_date, end_date, should_stop_func=should_stop_fetch)
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ•è·
            if isinstance(articles, dict) and articles.get('error') == 'no_session':
                logger.warning(f"âš ï¸  å‚æ•°å·²å¤±æ•ˆï¼Œå¼€å§‹é‡æ–°æ•è·...")
                
                # æ ‡è®°å‚æ•°å¤±æ•ˆ
                invalidate_parameters(biz)
                
                # è§¦å‘é‡æ–°æ•è·
                from api_server import ProxyManager
                if ProxyManager.start_proxy_and_capture(article_url, biz=biz, timeout=120):
                    time.sleep(2)
                    
                    # é‡æ–°è·å–å‚æ•°
                    params = get_valid_parameters(biz)
                    if params:
                        # é‡è¯•è·å–æ–‡ç« åˆ—è¡¨
                        articles = fetch_articles_with_params(biz, params, start_date, end_date, should_stop_func=should_stop_fetch)
                        
                        if isinstance(articles, dict) and articles.get('error'):
                            return jsonify({
                                'success': False,
                                'error': 'é‡æ–°æ•è·åä»ç„¶æ— æ³•è·å–æ–‡ç« åˆ—è¡¨'
                            }), 500
                    else:
                        return jsonify({
                            'success': False,
                            'error': 'å‚æ•°é‡æ–°æ•è·å¤±è´¥'
                        }), 500
                else:
                    return jsonify({
                        'success': False,
                        'error': 'å‚æ•°æ•è·å¤±è´¥ï¼Œè¯·ç¡®ä¿å¾®ä¿¡å·²æ­£å¸¸è¿è¡Œ',
                        'need_recapture': True
                    }), 500
                    
                # é‡æ–°åˆ›å»º ArticlesInfo
                articles_info = ArticlesInfo(
                    appmsg_token=params['appmsg_token'],
                    cookie=params['cookie']
                )
            
            # å¦‚æœæœ‰æ–°æ–‡ç« ï¼Œè·å–è¯¦æƒ…å¹¶ä¿å­˜
            new_articles_count = 0
            if articles and not (isinstance(articles, dict) and articles.get('error')):
                logger.info(f"   å‘ç° {len(articles)} ç¯‡æ–°æ–‡ç« ï¼Œå¼€å§‹è·å–è¯¦æƒ…...")
                new_articles_count = len(articles)
                
                # âœ… å…³é”®ï¼šå°†æ•°æ®åº“å‚æ•°å†™å…¥new_wechat_config.pyï¼Œä¾›download_full_htmlä½¿ç”¨
                _write_params_to_config(params)
                
                # æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®å¹¶ä¿å­˜
                import requests as req
                
                for i, article in enumerate(articles, 1):
                    try:
                        article_url_item = article.get('url', '')
                        article_title = article.get('title', '')
                        
                        logger.info(f"   [{i}/{len(articles)}] å¤„ç†: {article_title[:30]}...")
                        
                        # è½¬æ¢çŸ­é“¾æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        final_url = article_url_item
                        if '//mp.weixin.qq.com/s?' not in article_url_item and '//mp.weixin.qq.com/s/' in article_url_item:
                            try:
                                resp = req.get(article_url_item, headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=False, timeout=10)
                                if resp.status_code in [301, 302]:
                                    loc = resp.headers.get('Location')
                                    if loc:
                                        final_url = loc
                                else:
                                    # å°è¯•ä»HTMLä¸­æå–
                                    resp = req.get(article_url_item, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                                    content = resp.text
                                    urls = re.findall(r'var\s+msg_link\s*=\s*"([^"]+)"', content)
                                    if urls:
                                        final_url = urls[0].replace('\\/', '/')
                            except:
                                pass
                        
                        # è§£ç HTMLå®ä½“ (&amp; -> &) - å¿…é¡»åœ¨è·å–ç»Ÿè®¡æ•°æ®ä¹‹å‰ï¼
                        import html
                        final_url = html.unescape(final_url) if final_url else final_url
                        article_url_item = html.unescape(article_url_item) if article_url_item else article_url_item
                        
                        # âœ… ä½¿ç”¨download_full_html_with_statsä¸‹è½½HTMLå¹¶æå–ç»Ÿè®¡æ•°æ®
                        logger.info(f"      ğŸ“Š æ­£åœ¨ä¸‹è½½å®Œæ•´HTMLå¹¶æå–ç»Ÿè®¡æ•°æ®...")
                        logger.info(f"         URL: {final_url[:100]}...")
                        
                        download_result = download_full_html_with_stats(
                            final_url,
                            article_title,
                            article.get('publish_date'),
                            account_name=account_name,
                            output_dir="articles_html"
                        )
                        
                        html_file_path = download_result.get('filepath', '')
                        stats = download_result.get('stats', {})
                        
                        # è¯¦ç»†è®°å½•ç»Ÿè®¡æ•°æ®å“åº”
                        if stats:
                            logger.info(f"      ğŸ“Š ä»HTMLæå–çš„ç»Ÿè®¡æ•°æ®:")
                            logger.info(f"         read_num: {stats.get('read_num')}")
                            logger.info(f"         old_like_count: {stats.get('old_like_count')}")
                            logger.info(f"         like_count: {stats.get('like_count')}")
                            logger.info(f"         share_count: {stats.get('share_count')}")
                            logger.info(f"         comment_count: {stats.get('comment_count')}")
                        else:
                            logger.warning(f"      âš ï¸  æœªèƒ½ä»HTMLæå–ç»Ÿè®¡æ•°æ®")
                        
                        # âœ… è·å–ç•™è¨€å¹¶æ³¨å…¥åˆ°HTML
                        if html_file_path and os.path.exists(html_file_path):
                            try:
                                logger.info(f"      ğŸ’¬ æ­£åœ¨è·å–ç•™è¨€...")
                                from get_comments_improved import get_comment_id_from_html
                                from inject_comments_dom import inject_comments_direct_render
                                import urllib.parse
                                
                                # 1. ä»å·²ä¸‹è½½çš„HTMLä¸­æå–comment_id
                                with open(html_file_path, 'r', encoding='utf-8') as f:
                                    downloaded_html = f.read()
                                
                                comment_id = get_comment_id_from_html(downloaded_html)
                                
                                if comment_id:
                                    # 2. æå–URLå‚æ•°
                                    parsed = urllib.parse.urlparse(final_url)
                                    url_params = urllib.parse.parse_qs(parsed.query)
                                    __biz = url_params.get('__biz', [''])[0]
                                    idx = url_params.get('idx', ['1'])[0]
                                    
                                    # 3. æ„å»ºç•™è¨€APIè¯·æ±‚
                                    comment_api_params = {
                                        'action': 'getcomment',
                                        '__biz': __biz,
                                        'idx': idx,
                                        'comment_id': comment_id,
                                        'limit': '100',
                                        'uin': params.get('uin', ''),
                                        'key': params.get('key', ''),
                                        'pass_ticket': params.get('pass_ticket', ''),
                                        'appmsg_token': params.get('appmsg_token', '')
                                    }
                                    
                                    comment_url = "https://mp.weixin.qq.com/mp/appmsg_comment?" + urllib.parse.urlencode(comment_api_params)
                                    
                                    headers = {
                                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                                        'Cookie': params.get('cookie', '')
                                    }
                                    
                                    proxies = {"http": None, "https": None}
                                    comment_response = requests.get(comment_url, headers=headers, proxies=proxies, timeout=15)
                                    
                                    if comment_response.status_code == 200 and comment_response.text.strip():
                                        try:
                                            comments_data = comment_response.json()
                                            if comments_data and comments_data.get('elected_comment'):
                                                inject_comments_direct_render(html_file_path, comments_data)
                                            else:
                                                logger.info(f"      â„¹ï¸  è¯¥æ–‡ç« æ²¡æœ‰ç²¾é€‰ç•™è¨€")
                                        except:
                                            logger.info(f"      â„¹ï¸  ç•™è¨€APIè¿”å›éJSONæ ¼å¼")
                                    else:
                                        logger.info(f"      â„¹ï¸  ç•™è¨€APIè¯·æ±‚å¤±è´¥")
                                else:
                                    logger.info(f"      â„¹ï¸  è¯¥æ–‡ç« æœªå¼€å¯ç•™è¨€åŠŸèƒ½")
                            except Exception as e:
                                logger.warning(f"      âš ï¸  ç•™è¨€è·å–/æ³¨å…¥å¤±è´¥: {e}")
                        
                        # è¯»å–æœ¬åœ°HTMLæ–‡ä»¶ï¼ˆåŒ…å«å·²æ³¨å…¥çš„ç•™è¨€ï¼‰
                        html_content = None
                        if html_file_path and os.path.exists(html_file_path):
                            try:
                                with open(html_file_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                logger.info(f"      âœ… HTMLå·²ä¸‹è½½ ({len(html_content)} å­—èŠ‚)")
                            except Exception as e:
                                logger.warning(f"      âš ï¸  è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {e}")
                        else:
                            logger.warning(f"      âš ï¸  HTMLä¸‹è½½å¤±è´¥")
                        
                        # ä»HTMLæå–æ ‡é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if html_content and not article_title:
                            import re
                            title_match = re.search(r'<meta\s+property="og:title"\s+content="([^"]+)"', html_content)
                            if title_match:
                                article_title = title_match.group(1).strip()
                        
                        # æ¸…ç†ç»Ÿè®¡æ•°æ®ï¼šå°†ç©ºå€¼è½¬æ¢ä¸ºNone
                        def clean_stat(value):
                            if value is None or value == '' or value == 'N/A':
                                return None
                            try:
                                return int(value)
                            except (ValueError, TypeError):
                                return None
                        
                        article_data = {
                            'biz': biz,
                            'url': final_url,
                            'short_url': article_url_item if article_url_item != final_url else None,
                            'title': article_title,
                            'html_content': html_content,
                            'publish_date': article.get('publish_date'),
                            'read_count': clean_stat(stats.get('read_num')),
                            'old_like_count': clean_stat(stats.get('old_like_count')),
                            'like_count': clean_stat(stats.get('like_count')),
                            'share_count': clean_stat(stats.get('share_count')),
                            'comment_count': clean_stat(stats.get('comment_count')),
                            'local_html_path': html_file_path
                        }
                        
                        # ç›´æ¥æ’å…¥æ–°æ–‡ç« ï¼ˆå‰é¢çš„å¢é‡é€»è¾‘å·²ç»ä¿è¯äº†åªè·å–æ–°æ–‡ç« ï¼‰
                        logger.info(f"      å‡†å¤‡ä¿å­˜:")
                        logger.info(f"        æ ‡é¢˜: {article_title}")
                        logger.info(f"        URL: {final_url}")  # å®Œæ•´URL
                        logger.info(f"        çŸ­URL: {article_url_item if article_url_item else 'None'}")  # å®Œæ•´çŸ­URL
                        
                        from database import get_db_session
                        from models import Article as ArticleModel
                        
                        with get_db_session() as db_session:
                            new_article = ArticleModel(
                                biz=article_data['biz'],
                                url=article_data['url'],
                                short_url=article_data.get('short_url'),
                                title=article_data.get('title'),
                                html_content=article_data.get('html_content'),
                                publish_date=article_data.get('publish_date'),
                                read_count=article_data.get('read_count'),
                                old_like_count=article_data.get('old_like_count'),
                                like_count=article_data.get('like_count'),
                                share_count=article_data.get('share_count'),
                                comment_count=article_data.get('comment_count'),
                                local_html_path=article_data.get('local_html_path')
                            )
                            db_session.add(new_article)
                            db_session.flush()
                            logger.info(f"      âœ… ä¿å­˜æˆåŠŸ (ID: {new_article.id})")



                        
                        # é¿å…è¯·æ±‚è¿‡å¿«ï¼ˆå‚è€ƒsmart_batch_auto.pyï¼‰
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.warning(f"   âš ï¸  å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                        continue
            
            if new_articles_count > 0:
                logger.info(f"âœ… æˆåŠŸè·å–å¹¶ä¿å­˜ {new_articles_count} ç¯‡æ–°æ–‡ç« ")
            else:
                logger.info(f"âœ… æ²¡æœ‰å‘ç°æ–°æ–‡ç« ï¼ˆå·²å…¨éƒ¨è¦†ç›–ï¼‰")
            
            # 6. ä»æ•°æ®åº“æŸ¥è¯¢æœ€ç»ˆç»“æœï¼ˆæŒ‰è¿‡æ»¤æ¡ä»¶ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œé‡æ–°æŸ¥è¯¢ä»¥è·å–åŒ…æ‹¬æ—§æ–‡ç« åœ¨å†…çš„æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« 
            final_articles = get_articles_by_filters(biz, start_date, end_date, min_read_count)
            
            # æ‰‹åŠ¨æˆªå– limitï¼ˆå› ä¸º get_articles_by_filters æ²¡æœ‰ limit å‚æ•°ï¼‰
            final_articles = final_articles[:limit]
            
            return jsonify({
                'success': True,
                'data': {
                    'account_name': account_name,
                    'biz': biz,
                    'from_cache': new_articles_count == 0, # å¦‚æœæ²¡æœ‰æ–°æ–‡ç« ï¼Œè¯´æ˜å®Œå…¨æ¥è‡ªç¼“å­˜
                    'total_saved': new_articles_count, # æœ¬æ¬¡æ–°ä¿å­˜çš„æ•°é‡
                    'total': len(final_articles),     # è¿”å›ç»™å‰ç«¯çš„æ€»æ•°
                    'articles': final_articles
                }
            })
            
        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨å¾®ä¿¡APIå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({
                'success': False,
                'error': f'è°ƒç”¨å¾®ä¿¡APIå¤±è´¥: {str(e)}'
            }), 500
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500